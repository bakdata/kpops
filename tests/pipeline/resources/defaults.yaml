kubernetes-app:
  name: "${component_type}"
  namespace: example-namespace

kafka-app:
  app:
    streams:
      brokers: "${broker}"
      schema_registry_url: "${schema_registry_url}"
  version: "2.4.2"

producer: {} # inherits from kafka

streams-app: # inherits from kafka
  app:
    streams:
      config:
        large.message.id.generator: com.bakdata.kafka.MurmurHashIdGenerator
  to:
    topics:
      ${error_topic_name}:
        type: error
        valueSchema: com.bakdata.kafka.DeadLetter
        partitions_count: 1
        configs:
          cleanup.policy: compact,delete

scheduled-producer:
  app:
    image: "example-registry/fake-image"
    imageTag: "0.0.1"
  to:
    topics:
      ${output_topic_name}:
        type: output
        valueSchema: com.bakdata.fake.Produced
        partitions_count: 12
        configs:
          cleanup.policy: compact,delete
    models:
      "com/bakdata/kafka/fake": 1.0.0

converter:
  app:
    resources:
      limits:
        memory: 3G
    autoscaling:
      enabled: true
      consumergroup: converter-${output_topic_name}
      maxReplicas: 1
      lagThreshold: "10000"
  to:
    topics:
      ${output_topic_name}:
        type: output
        partitions_count: 50
        configs:
          retention.ms: "-1"
          cleanup.policy: compact,delete
      ${error_topic_name}:
        type: error
        valueSchema: com.bakdata.kafka.DeadLetter
        partitions_count: 10
        configs:
          cleanup.policy: compact,delete

filter:
  app:
    image: "fake-registry/filter"
    imageTag: "2.4.1"
    autoscaling:
      enabled: true
      maxReplicas: 1
      lagThreshold: "10000"
      consumergroup: filter-${output_topic_name}
      topics:
        - "${output_topic_name}"
  to:
    topics:
      ${output_topic_name}:
        type: output
        partitions_count: 50
        configs:
          retention.ms: "-1"

should-inflate:
  app:
    image: "fake-registry/filter"
    imageTag: "2.4.1"
    autoscaling:
      enabled: true
      maxReplicas: 1
      lagThreshold: "10000"
      consumergroup: filter-${output_topic_name}
      topics:
        - "${output_topic_name}"
  to:
    topics:
      ${output_topic_name}:
        type: output
        partitions_count: 50
        configs:
          retention.ms: "-1"

kafka-connect:
  name: "sink-connector"
  namespace: "example-namespace"
  app:
    batch.size: "2000"
    behavior.on.malformed.documents: "warn"
    behavior.on.null.values: "delete"
    connection.compression: "true"
    connector.class: "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector"
    key.ignore: "false"
    linger.ms: "5000"
    max.buffered.records: "20000"
    name: "sink-connector"
    read.timeout.ms: "120000"
    tasks.max: "1"
