# Base Kubernetes App
- type: kubernetes-app
  name: kubernetes-app # required
  namespace: namespace # required
  # `app` contains application-specific settings, hence it does not have a rigid
  # structure. The fields below are just an example.
  app: # required
    image: exampleImage # Example
    debug: false # Example
    commandLine: {} # Example
  # Topic(s) from which the component will read input
  from:
    topics: # required
      ${pipeline_name}-input-topic:
        type: input # required
        # role: topic-role # only used if type is `extra` or `extra-pattern`
  # Topic(s) into which the component will write output
  to:
    topics: # required
      ${pipeline_name}-output-topic:
        type: output # required
        # role: topic-role # only used if type is `extra`
        key_schema: key-schema # must implement SchemaProvider to use
        value_schema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs: # https://kafka.apache.org/documentation/#topicconfigs
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Helm repository configuration
  repoConfig:
    repositoryName: my-repo # required
    url: https://bakdata.github.io/ # required
    repoAuthFlags:
      username: user
      password: pass
      ca_file: /home/user/path/to/ca-file
      insecure_skip_tls_verify: false
  version: "1.0.0" # Helm chart version
# Base component for Kafka-based components.
# Producer or streaming apps should inherit from this class.
- type: kafka-app # required
  name: kafka-app # required
  namespace: namespace # required
  # `app` can contain application-specific settings, hence  the user is free to
  # add the key-value pairs they need.
  app: # required
    streams: # required
      brokers: ${brokers} # required
      schemaRegistryUrl: ${schema_registry_url}
    nameOverride: override-with-this-name # kafka-app-specific
    imageTag: "1.0.0" # Example values that are shared between streams-app and producer-app
  # Topic(s) from which the component will read input
  # from: # Not advised to ever be used in `kafka-app` as it isn't supported by
  # `producer`
  # Topic(s) into which the component will write output
  to:
    topics: # required
      ${pipeline_name}-output-topic:
        type: output # required
        # role: topic-role # only used if type is `extra`
        # Currently KPOps only supports Avro schemas.
        key_schema: key-schema # must implement SchemaProvider to use
        value_schema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs:
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Helm repository configuration, the default value is given here
  repoConfig:
    repositoryName: bakdata-streams-bootstrap # required
    url: https://bakdata.github.io/streams-bootstrap/ # required
    repo_auth_flags:
      username: user
      password: pass
      ca_file: /home/user/path/to/ca-file
      insecure_skip_tls_verify: false
  version: "2.7.0" # Helm chart version
# StreamsApp component that configures a streams bootstrap app.
# More documentation on StreamsApp: https://github.com/bakdata/streams-bootstrap
- type: streams-app # required
  name: streams-app # required
  namespace: namespace # required
  # No arbitrary keys are allowed under `app`here
  # Allowed configs:
  # https://github.com/bakdata/streams-bootstrap/tree/master/charts/streams-app
  app: # required
    # Streams Bootstrap streams section
    streams: # required, streams-app-specific
      brokers: ${brokers} # required
      schemaRegistryUrl: ${schema_registry_url}
      inputTopics:
        - topic1
        - topic2
      outputTopic: output-topic
      inputPattern: input-pattern
      extraInputTopics:
        input_role1:
          - input_topic1
          - input_topic2
        input_role2:
          - input_topic3
          - input_topic4
      extraInputPatterns:
        pattern_role1: input_pattern1
      extraOutputTopics:
        output_role1: output_topic1
        output_role2: output_topic2
      errorTopic: error-topic
      config:
        my.streams.config: my.value
    nameOverride: override-with-this-name # streams-app-specific
    autoscaling: # streams-app-specific
      consumerGroup: consumer-group # required
      lagThreshold: 0 # Average target value to trigger scaling actions.
      enabled: false # Whether to enable auto-scaling using KEDA.
      # This is the interval to check each trigger on.
      # https://keda.sh/docs/2.9/concepts/scaling-deployments/#pollinginterval
      pollingInterval: 30
      # The period to wait after the last trigger reported active before scaling
      #  the resource back to 0. https://keda.sh/docs/2.9/concepts/scaling-deployments/#cooldownperiod
      cooldownPeriod: 300
      # The offset reset policy for the consumer if the the consumer group is
      # not yet subscribed to a partition.
      offsetResetPolicy: earliest
      # This setting is passed to the HPA definition that KEDA will create for a
      # given resource and holds the maximum number of replicas of the target resouce.
      # https://keda.sh/docs/2.9/concepts/scaling-deployments/#maxreplicacount
      maxReplicas: 1
      # Minimum number of replicas KEDA will scale the resource down to.
      # https://keda.sh/docs/2.7/concepts/scaling-deployments/#minreplicacount
      minReplicas: 0
      # If this property is set, KEDA will scale the resource down to this
      # number of replicas.
      # https://keda.sh/docs/2.9/concepts/scaling-deployments/#idlereplicacount
      idleReplicas: 0
      topics: # List of auto-generated Kafka Streams topics used by the streams app.
        - topic1
        - topic2
  # Topic(s) from which the component will read input
  from:
    topics: # required
      ${pipeline_name}-input-topic:
        type: input # required
        # role: topic-role # only used if type is `extra` or `extra-pattern`
  # Topic(s) into which the component will write output
  to:
    topics: # required
      ${pipeline_name}-output-topic:
        type: output # required
        # role: topic-role # only used if type is `extra`
        key_schema: key-schema # must implement SchemaProvider to use
        value_schema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs:
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Helm repository configuration, the default value is given here
  repoConfig:
    repositoryName: bakdata-streams-bootstrap # required
    url: https://bakdata.github.io/streams-bootstrap/ # required
    repo_auth_flags:
      username: user
      password: pass
      ca_file: /home/user/path/to/ca-file
      insecure_skip_tls_verify: false
  version: "2.7.0" # Helm chart version
# Holds configuration to use as values for the streams bootstrap producer Helm
# chart.
# More documentation on ProducersApp:
# https://github.com/bakdata/streams-bootstrap
- type: producer
  name: producer # required
  namespace: namespace # required
  # Allowed configs:
  # https://github.com/bakdata/streams-bootstrap/tree/master/charts/producer-app
  app: # required
    streams: # required, producer-specific
      brokers: ${brokers} # required
      schemaRegistryUrl: ${schema_registry_url}
      outputTopic: output_topic
      extraOutputTopics:
        output_role1: output_topic1
        output_role2: output_topic2
    nameOverride: override-with-this-name # kafka-app-specific
  # Topic(s) into which the component will write output
  to:
    topics: # required
      ${pipeline_name}-output-topic:
        type: output # required
        # role: topic-role # only used if type is `extra`
        key_schema: key-schema # must implement SchemaProvider to use
        value_schema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs: # https://kafka.apache.org/documentation/#topicconfigs
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Helm repository configuration, the default value is given here
  repoConfig:
    repositoryName: bakdata-streams-bootstrap # required
    url: https://bakdata.github.io/streams-bootstrap/ # required
    repo_auth_flags:
      username: user
      password: pass
      ca_file: /home/user/path/to/ca-file
      insecure_skip_tls_verify: false
  version: "2.7.0" # Helm chart version
# Kafka source connector
- type: kafka-source-connector
  name: kafka-source-connector # required
  namespace: namespace # required, clean up jobs for the connector will run here
  # `app` contains application-specific settings, hence it does not have a rigid
  # structure. The fields below are just an example. Extensive documentation on
  # source connectors: https://kafka.apache.org/documentation/#sourceconnectconfigs
  app: # required
    tasks.max: 1
    errors.tolerance: none
  # Topic(s) into which the component will write output
  to:
    topics: # required
      ${pipeline_name}-output-topic:
        type: output # required
        # role: topic-role # only used if type is `extra`
        key_schema: key-schema # must implement SchemaProvider to use
        value_schema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs: # https://kafka.apache.org/documentation/#topicconfigs
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Helm repository configuration
  repoConfig:
    repositoryName: my-repo # required
    url: https://bakdata.github.io/kafka-connect-resetter/ # required
    repoAuthFlags:
      username: user
      password: pass
      ca_file: /home/user/path/to/ca-file
      insecure_skip_tls_verify: false
  version: "1.0.4" # Helm chart version
  # Overriding Kafka Connect Resetter Helm values. E.g. to override the
  # Image Tag etc.
  resetterValues:
    imageTag: "1.2.3"
  # offset.storage.topic
  # https://kafka.apache.org/documentation/#connect_running
  offsetTopic: offset_topic
# Kafka sink connector
- type: kafka-sink-connector
  name: kafka-sink-connector # required,
  namespace: namespace # required, clean up jobs for the connector will run here
  # `app` contains application-specific settings, hence it does not have a rigid
  # structure. The fields below are just an example. Extensive documentation on
  # sink connectors: https://kafka.apache.org/documentation/#sinkconnectconfigs
  app: # required
    tasks.max: 1
  # Topic(s) from which the component will read input
  from:
    topics: # required
      ${pipeline_name}-input-topic:
        type: input # required
        # role: topic-role # only used if type is `extra`
  # Topic(s) into which the component will write output
  to:
    topics: # required
      ${pipeline_name}-output-topic:
        type: error # required
        key_schema: key-schema # must implement SchemaProvider to use
        value_schema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs: # https://kafka.apache.org/documentation/#topicconfigs
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Helm repository configuration for resetter
  repoConfig:
    repositoryName: my-repo # required
    url: https://bakdata.github.io/kafka-connect-resetter/ # required
    repoAuthFlags:
      username: user
      password: pass
      ca_file: /home/user/path/to/ca-file
      insecure_skip_tls_verify: false
  version: "1.0.4" # Helm chart version
  # Overriding Kafka Connect Resetter Helm values. E.g. to override the
  # Image Tag etc.
  resetterValues:
    imageTag: "1.2.3"
