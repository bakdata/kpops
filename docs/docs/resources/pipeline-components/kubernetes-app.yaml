# Base Kubernetes App
- type: kubernetes-app
  name: kubernetes-app # required
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Topic(s) from which the component will read input
  from: # Must not be null
    topics: # read from topic
      ${pipeline_name}-input-topic:
        type: input # required
        # role: topic-role # only used if type is `extra` or `extra-pattern`
    components: # read from specific component
      producer-app:
        type: input # required
  # Topic(s) into which the component will write output
  to:
    topics:
      ${pipeline_name}-output-topic:
        type: output # required
        # role: topic-role # only used if type is `extra`
        key_schema: key-schema # must implement SchemaProvider to use
        value_schema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs: # https://kafka.apache.org/documentation/#topicconfigs
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  namespace: namespace # required
  # `app` contains application-specific settings, hence it does not have a rigid
  # structure. The fields below are just an example.
  app: # required
    image: exampleImage # Example
    debug: false # Example
    commandLine: {} # Example
  # Topic(s) from which the component will read input
  from:
    topics: # required
      ${pipeline_name}-input-topic:
        type: input # Implied when role is NOT specified, alternatives of `type: null` or `role: null` also work
      ${pipeline_name}-extra-topic:
        role: topic-role # Implies `type` to be extra
      ${pipeline_name}-input-pattern-topic:
        type: pattern # Implied to be an input pattern if `role` is undefined
      ${pipeline_name}-extra-pattern-topic:
        type: pattern # Implied to be an extra pattern if `role` undefined
        role: some-role
    components: # read from specific component
      account-producer:
        type: output # Implied when role is NOT specified, alternatives of `type: null` or `role: null` also work
      other-producer:
        role: some-role # Implies `type` to be extra
      component-as-input-pattern:
        type: pattern # Implied to be an input pattern if `role` is undefined
      component-as-extra-pattern:
        type: pattern # Implied to be an extra pattern if `role` undefined
        role: some-role
  # Topic(s) into which the component will write output
  to:
    topics: # required
      ${pipeline_name}-output-topic:
        type: output # Implied when role is NOT specified
      ${pipeline_name}-extra-topic:
        role: topic-role # Implies `type` to be extra; Will throw an error if `type` is defined
      ${pipeline_name}-error-topic:
        type: error
        # Currently KPOps supports Avro and JSON schemas.
        keySchema: key-schema # must implement SchemaProvider to use
        valueSchema: value-schema
        partitions_count: 1
        replication_factor: 1
        configs: # https://kafka.apache.org/documentation/#topicconfigs
          cleanup.policy: compact
    models: # SchemaProvider is initiated with the values given here
      model: model
  # Pipeline prefix that will prefix every component name. If you wish to not
  # have any prefix you can specify an empty string.
  prefix: ${pipeline_name}-
  # Helm repository configuration (optional)
  # If not set the helm repo add will not be called. Useful when using local Helm charts
  repo_config:
    repository_name: bakdata-streams-bootstrap # required
    url: https://bakdata.github.io/streams-bootstrap/ # required
    repo_auth_flags:
      username: user
      password: pass
      ca_file: /home/user/path/to/ca-file
      insecure_skip_tls_verify: false
  version: "1.0.0" # Helm chart version
