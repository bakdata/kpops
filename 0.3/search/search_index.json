{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 User guide","title":"Overview"},{"location":"#overview","text":"User guide","title":"Overview"},{"location":"user/","text":"KPOps User guide \u00b6 What is KPOps? \u00b6 KPOps is a CLI tool that allows deploying and destroying consecutive Kafka Streams applications, Producers, and Kafka Connectors using an easy-to-read and write pipeline definition. KPOps comes with various abstractions that can simplify configuring multiple pipelines and steps within pipelines by sharing configuration between different applications, like producers or streaming applications. In addition, Kafka apps and its resources can be reset or cleaned up again with a single command. User guide \u00b6 The user guide is split into three parts: Getting Started Examples References","title":"KPOps User guide"},{"location":"user/#kpops-user-guide","text":"","title":"KPOps User guide"},{"location":"user/#what-is-kpops","text":"KPOps is a CLI tool that allows deploying and destroying consecutive Kafka Streams applications, Producers, and Kafka Connectors using an easy-to-read and write pipeline definition. KPOps comes with various abstractions that can simplify configuring multiple pipelines and steps within pipelines by sharing configuration between different applications, like producers or streaming applications. In addition, Kafka apps and its resources can be reset or cleaned up again with a single command.","title":"What is KPOps?"},{"location":"user/#user-guide","text":"The user guide is split into three parts: Getting Started Examples References","title":"User guide"},{"location":"user/examples/","text":"Examples \u00b6 The examples showcase Kafka pipelines built with KPOps. ATM fraud pipeline is a pipeline for ATM fraud detection. This example shows you how to set up a Kafka pipeline with KPOps and deploy it to a Kubernetes cluster. It also uses a PostgreSQL database as data sink to store the processed results.","title":"Examples"},{"location":"user/examples/#examples","text":"The examples showcase Kafka pipelines built with KPOps. ATM fraud pipeline is a pipeline for ATM fraud detection. This example shows you how to set up a Kafka pipeline with KPOps and deploy it to a Kubernetes cluster. It also uses a PostgreSQL database as data sink to store the processed results.","title":"Examples"},{"location":"user/examples/atm-fraud-pipeline/","text":"ATM fraud detection pipeline \u00b6 ATM fraud is a demo pipeline for ATM fraud detection. The original by Confluent is written in KSQL and outlined in this blogpost . The one used in this example is re-built from scratch using bakdata 's streams-bootstrap library. What this will demonstrate \u00b6 Deploying a PostgreSQL database using Helm Deploying a pipeline using kpops Destroying a pipeline using kpops Prerequisites \u00b6 Completed all steps in the setup . Setup and deployment \u00b6 PostgreSQL \u00b6 Deploy PostgreSQL using the Bitnami Helm chart: Add the helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami && \\ helm repo update Install the PostgreSQL with helm: helm upgrade --install -f ./postgresql.yaml \\ --namespace kpops \\ postgresql bitnami/postgresql PostgreSQL Example Helm chart values ( postgresql.yaml ) auth : database : app_db enablePostgresUser : true password : AppPassword postgresPassword : StrongPassword username : app1 primary : persistence : enabled : false existingClaim : postgresql-data-claim volumePermissions : enabled : true ATM fraud detection example pipeline setup \u00b6 Port forwarding \u00b6 Before we deploy the pipeline, we need to forward the ports of kafka-rest-proxy and kafka-connect . Run the following commands in two different terminals. kubectl port-forward --namespace kpops service/k8kafka-cp-rest 8082 :8082 kubectl port-forward --namespace kpops service/k8kafka-cp-kafka-connect 8083 :8083 Deploying the ATM fraud detection pipeline \u00b6 Export environment variables in your terminal: export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Deploy the pipeline poetry run kpops deploy ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be deployed correctly. Check if the deployment is successful \u00b6 You can use the Streams Explorer to see the deployed pipeline. To do so, port-forward the service in a separate terminal session using the command below: kubectl port-forward -n kpops service/streams-explorer 8080 :8080 After that open http://localhost:8080 in your browser. You should be able to see pipeline shown in the image below: An overview of ATM fraud pipeline shown in Streams Explorer Attention Kafka Connect needs some time to set up the connector. Moreover, Streams Explorer needs a while to scrape the information from Kafka connect. Therefore, it might take a bit until you see the whole graph. Teardown resources \u00b6 PostrgreSQL \u00b6 PostgreSQL can be uninstalled by running the following command: helm --namespace kpops uninstall postgresql ATM fraud pipeline \u00b6 Export environment variables in your terminal. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Remove the pipeline poetry run kpops clean ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --verbose \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be destroyed correctly. Attention If you face any issues destroying this example see Teardown for manual deletion. Common errors \u00b6 deploy fails: Read the error message. Try to correct the mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . Run deploy --dry-run to avoid havig to clean again. If an error is dropped, start over from step 1. If the dry-run is succesful, run deploy . clean fails: Read the error message. Try to correct the indicated mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . If clean fails, follow the steps in teardown .","title":"ATM fraud detection pipeline"},{"location":"user/examples/atm-fraud-pipeline/#atm-fraud-detection-pipeline","text":"ATM fraud is a demo pipeline for ATM fraud detection. The original by Confluent is written in KSQL and outlined in this blogpost . The one used in this example is re-built from scratch using bakdata 's streams-bootstrap library.","title":"ATM fraud detection pipeline"},{"location":"user/examples/atm-fraud-pipeline/#what-this-will-demonstrate","text":"Deploying a PostgreSQL database using Helm Deploying a pipeline using kpops Destroying a pipeline using kpops","title":"What this will demonstrate"},{"location":"user/examples/atm-fraud-pipeline/#prerequisites","text":"Completed all steps in the setup .","title":"Prerequisites"},{"location":"user/examples/atm-fraud-pipeline/#setup-and-deployment","text":"","title":"Setup and deployment"},{"location":"user/examples/atm-fraud-pipeline/#postgresql","text":"Deploy PostgreSQL using the Bitnami Helm chart: Add the helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami && \\ helm repo update Install the PostgreSQL with helm: helm upgrade --install -f ./postgresql.yaml \\ --namespace kpops \\ postgresql bitnami/postgresql PostgreSQL Example Helm chart values ( postgresql.yaml ) auth : database : app_db enablePostgresUser : true password : AppPassword postgresPassword : StrongPassword username : app1 primary : persistence : enabled : false existingClaim : postgresql-data-claim volumePermissions : enabled : true","title":"PostgreSQL"},{"location":"user/examples/atm-fraud-pipeline/#atm-fraud-detection-example-pipeline-setup","text":"","title":"ATM fraud detection example pipeline setup"},{"location":"user/examples/atm-fraud-pipeline/#port-forwarding","text":"Before we deploy the pipeline, we need to forward the ports of kafka-rest-proxy and kafka-connect . Run the following commands in two different terminals. kubectl port-forward --namespace kpops service/k8kafka-cp-rest 8082 :8082 kubectl port-forward --namespace kpops service/k8kafka-cp-kafka-connect 8083 :8083","title":"Port forwarding"},{"location":"user/examples/atm-fraud-pipeline/#deploying-the-atm-fraud-detection-pipeline","text":"Export environment variables in your terminal: export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Deploy the pipeline poetry run kpops deploy ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be deployed correctly.","title":"Deploying the ATM fraud detection pipeline"},{"location":"user/examples/atm-fraud-pipeline/#check-if-the-deployment-is-successful","text":"You can use the Streams Explorer to see the deployed pipeline. To do so, port-forward the service in a separate terminal session using the command below: kubectl port-forward -n kpops service/streams-explorer 8080 :8080 After that open http://localhost:8080 in your browser. You should be able to see pipeline shown in the image below: An overview of ATM fraud pipeline shown in Streams Explorer Attention Kafka Connect needs some time to set up the connector. Moreover, Streams Explorer needs a while to scrape the information from Kafka connect. Therefore, it might take a bit until you see the whole graph.","title":"Check if the deployment is successful"},{"location":"user/examples/atm-fraud-pipeline/#teardown-resources","text":"","title":"Teardown resources"},{"location":"user/examples/atm-fraud-pipeline/#postrgresql","text":"PostgreSQL can be uninstalled by running the following command: helm --namespace kpops uninstall postgresql","title":"PostrgreSQL"},{"location":"user/examples/atm-fraud-pipeline/#atm-fraud-pipeline","text":"Export environment variables in your terminal. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Remove the pipeline poetry run kpops clean ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --verbose \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be destroyed correctly. Attention If you face any issues destroying this example see Teardown for manual deletion.","title":"ATM fraud pipeline"},{"location":"user/examples/atm-fraud-pipeline/#common-errors","text":"deploy fails: Read the error message. Try to correct the mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . Run deploy --dry-run to avoid havig to clean again. If an error is dropped, start over from step 1. If the dry-run is succesful, run deploy . clean fails: Read the error message. Try to correct the indicated mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . If clean fails, follow the steps in teardown .","title":"Common errors"},{"location":"user/getting-started/","text":"Getting started \u00b6 The getting started section gives you an overview of KPOps and how to work with it. In Setup KPOps , you learn how to deploy the necessary infrastructure for running Kafka pipelines and how to install the KPOps CLI. The Teardown covers the process of destroying everything you deployed in this guide.","title":"Getting started"},{"location":"user/getting-started/#getting-started","text":"The getting started section gives you an overview of KPOps and how to work with it. In Setup KPOps , you learn how to deploy the necessary infrastructure for running Kafka pipelines and how to install the KPOps CLI. The Teardown covers the process of destroying everything you deployed in this guide.","title":"Getting started"},{"location":"user/getting-started/setup/","text":"Setup KPOps \u00b6 In this part, you will set up KPOps. This includes: optionally creating a local Kubernetes cluster running Apache Kafka and Confluent's Schema Registry installing KPOps Prerequisites \u00b6 k3d (Version 5.4.6+) and Docker (Version >= v20.10.5) or an existing Kubernetes cluster (>= 1.21.0) kubectl (Compatible with server version 1.21.0) Helm (Version 3.8.0+) Setup Kubernetes with k3d \u00b6 If you don't have access to an existing Kubernetes cluster, this section will guide you through creating a local cluster. We recommend the lightweight Kubernetes distribution k3s for this. k3d is a wrapper around k3s in Docker that lets you get started fast. You can install k3d with its installation script: wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/v5.4.6/install.sh | bash For other ways of installing k3d, you can have a look at their installation guide . The Kafka deployment needs a modified Docker image. In that case the image is built and pushed to a Docker registry that holds it. If you do not have access to an existing Docker registry, you can use k3d's Docker registry: k3d registry create kpops-registry.localhost --port 12345 Now you can create a new cluster called kpops that uses the previously created Docker registry: k3d cluster create kpops --k3s-arg \"--no-deploy=traefik@server:*\" --registry-use k3d-kpops-registry.localhost:12345 Note Creating a new k3d cluster automatically configures kubectl to connect to the local cluster by modifying your ~/.kube/config . In case you manually set the KUBECONFIG variable or don't want k3d to modify your config, k3d offers many other options . You can check the cluster status with kubectl get pods -n kube-system . If all returned elements have a STATUS of Running or Completed , then the cluster is up and running. Deploy Kafka \u00b6 Kafka is an open-source data streaming platform. More information about Kafka can be found in the documentation . To deploy Kafka, this guide uses Confluent's Helm chart . To allow connectivity to other systems Kafka Connect needs to be extended with drivers. You can install a JDBC driver for Kafka Connect by creating a new Docker image: Create a Dockerfile with the following content: FROM confluentinc/cp-kafka-connect:7.1.3 RUN confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.6.0 Build and push the modified image to your private Docker registry: docker build . --tag localhost:12345/kafka-connect-jdbc:7.1.3 && \\ docker push localhost:12345/kafka-connect-jdbc:7.1.3 Detailed instructions on building, tagging and pushing a docker image can be found in Docker docs . Add Confluent's Helm chart repository and update the index: helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/ && helm repo update Install Kafka, Zookeeper, Confluent's Schema Registry, Kafka Rest Proxy, and Kafka Connect. A single Helm chart installs all five components. Below you can find an example for the --values ./kafka.yaml file configuring the deployment accordingly. Deploy the services: helm upgrade \\ --install \\ --version 0 .6.1 \\ --values ./kafka.yaml \\ --namespace kpops \\ --create-namespace \\ --wait \\ k8kafka confluentinc/cp-helm-charts Kafka Helm chart values ( kafka.yaml ) An example value configuration for Confluent's Helm chart. This configuration deploys a single Kafka Broker, a Schema Registry, Zookeeper, Kafka Rest Proxy, and Kafka Connect with minimal resources. cp-zookeeper : enabled : true servers : 1 imageTag : 7.1.3 heapOptions : \"-Xms124M -Xmx124M\" overrideGroupId : k8kafka fullnameOverride : \"k8kafka-cp-zookeeper\" resources : requests : cpu : 50m memory : 0.2G limits : cpu : 250m memory : 0.2G prometheus : jmx : enabled : false cp-kafka : enabled : true brokers : 1 imageTag : 7.1.3 podManagementPolicy : Parallel configurationOverrides : \"auto.create.topics.enable\" : false \"offsets.topic.replication.factor\" : 1 \"transaction.state.log.replication.factor\" : 1 \"transaction.state.log.min.isr\" : 1 resources : requests : cpu : 50m memory : 0.5G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false persistence : enabled : false cp-schema-registry : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-schema-registry\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.25G prometheus : jmx : enabled : false cp-kafka-connect : enabled : true replicaCount : 1 image : k3d-kpops-registry.localhost:12345/kafka-connect-jdbc imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-kafka-connect\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" heapOptions : \"-Xms256M -Xmx256\" resources : requests : cpu : 500m memory : 0.25G limits : cpu : 500m memory : 0.25G configurationOverrides : \"consumer.max.poll.records\" : \"10\" \"consumer.max.poll.interval.ms\" : \"900000\" \"config.storage.replication.factor\" : \"1\" \"offset.storage.replication.factor\" : \"1\" \"status.storage.replication.factor\" : \"1\" cp-schema-registry : url : http://k8kafka-cp-schema-registry:8081 prometheus : jmx : enabled : false cp-kafka-rest : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-rest\" heapOptions : \"-Xms256M -Xmx256M\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false cp-ksql-server : enabled : false cp-control-center : enabled : false Deploy Streams Explorer \u00b6 Streams Explorer allows examining Apache Kafka data pipelines in a Kubernetes cluster including the inspection of schemas and monitoring of metrics. First, add the Helm repository: helm repo add streams-explorer https://bakdata.github.io/streams-explorer && \\ helm repo update Below you can find an example for the --values ./streams-explorer.yaml file configuring the deployment accordingly. Now, deploy the service: helm upgrade \\ --install \\ --version 0 .2.3 \\ --values ./streams-explorer.yaml \\ --namespace kpops \\ streams-explorer streams-explorer/streams-explorer Streams Explorer Helm chart values ( streams-explorer.yaml ) An example value configuration for Steams Explorer Helm chart. imageTag : \"v2.1.2\" config : K8S__deployment__cluster : true SCHEMAREGISTRY__url : http://k8kafka-cp-schema-registry.kpops.svc.cluster.local:8081 KAFKACONNECT__url : http://k8kafka-cp-kafka-connect.kpops.svc.cluster.local:8083 resources : requests : cpu : 200m memory : 300Mi limits : cpu : 200m memory : 300Mi Check the status of your deployments \u00b6 Now we will check if all the pods are running in our namespace. You can list all pods in the namespace with this command: kubectl --namespace kpops get pods Then you should see the following out put in your terminal. NAME READY STATUS RESTARTS AGE k8kafka-cp-kafka-connect-8fc7d544f-8pjnt 1 /1 Running 0 15m k8kafka-cp-zookeeper-0 1 /1 Running 0 15m k8kafka-cp-kafka-0 1 /1 Running 0 15m k8kafka-cp-schema-registry-588f8c65db-jdwbq 1 /1 Running 0 15m k8kafka-cp-rest-6bbfd7b645-nwkf8 1 /1 Running 0 15m streams-explorer-54db878c67-s8wbz 1 /1 Running 0 15m Pay attention to the STATUS row. The pods should have a status of Running . Install KPOps \u00b6 KPOps comes as a PyPI package . You can install it with pip : pip install kpops","title":"Setup KPOps"},{"location":"user/getting-started/setup/#setup-kpops","text":"In this part, you will set up KPOps. This includes: optionally creating a local Kubernetes cluster running Apache Kafka and Confluent's Schema Registry installing KPOps","title":"Setup KPOps"},{"location":"user/getting-started/setup/#prerequisites","text":"k3d (Version 5.4.6+) and Docker (Version >= v20.10.5) or an existing Kubernetes cluster (>= 1.21.0) kubectl (Compatible with server version 1.21.0) Helm (Version 3.8.0+)","title":"Prerequisites"},{"location":"user/getting-started/setup/#setup-kubernetes-with-k3d","text":"If you don't have access to an existing Kubernetes cluster, this section will guide you through creating a local cluster. We recommend the lightweight Kubernetes distribution k3s for this. k3d is a wrapper around k3s in Docker that lets you get started fast. You can install k3d with its installation script: wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/v5.4.6/install.sh | bash For other ways of installing k3d, you can have a look at their installation guide . The Kafka deployment needs a modified Docker image. In that case the image is built and pushed to a Docker registry that holds it. If you do not have access to an existing Docker registry, you can use k3d's Docker registry: k3d registry create kpops-registry.localhost --port 12345 Now you can create a new cluster called kpops that uses the previously created Docker registry: k3d cluster create kpops --k3s-arg \"--no-deploy=traefik@server:*\" --registry-use k3d-kpops-registry.localhost:12345 Note Creating a new k3d cluster automatically configures kubectl to connect to the local cluster by modifying your ~/.kube/config . In case you manually set the KUBECONFIG variable or don't want k3d to modify your config, k3d offers many other options . You can check the cluster status with kubectl get pods -n kube-system . If all returned elements have a STATUS of Running or Completed , then the cluster is up and running.","title":"Setup Kubernetes with k3d"},{"location":"user/getting-started/setup/#deploy-kafka","text":"Kafka is an open-source data streaming platform. More information about Kafka can be found in the documentation . To deploy Kafka, this guide uses Confluent's Helm chart . To allow connectivity to other systems Kafka Connect needs to be extended with drivers. You can install a JDBC driver for Kafka Connect by creating a new Docker image: Create a Dockerfile with the following content: FROM confluentinc/cp-kafka-connect:7.1.3 RUN confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.6.0 Build and push the modified image to your private Docker registry: docker build . --tag localhost:12345/kafka-connect-jdbc:7.1.3 && \\ docker push localhost:12345/kafka-connect-jdbc:7.1.3 Detailed instructions on building, tagging and pushing a docker image can be found in Docker docs . Add Confluent's Helm chart repository and update the index: helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/ && helm repo update Install Kafka, Zookeeper, Confluent's Schema Registry, Kafka Rest Proxy, and Kafka Connect. A single Helm chart installs all five components. Below you can find an example for the --values ./kafka.yaml file configuring the deployment accordingly. Deploy the services: helm upgrade \\ --install \\ --version 0 .6.1 \\ --values ./kafka.yaml \\ --namespace kpops \\ --create-namespace \\ --wait \\ k8kafka confluentinc/cp-helm-charts Kafka Helm chart values ( kafka.yaml ) An example value configuration for Confluent's Helm chart. This configuration deploys a single Kafka Broker, a Schema Registry, Zookeeper, Kafka Rest Proxy, and Kafka Connect with minimal resources. cp-zookeeper : enabled : true servers : 1 imageTag : 7.1.3 heapOptions : \"-Xms124M -Xmx124M\" overrideGroupId : k8kafka fullnameOverride : \"k8kafka-cp-zookeeper\" resources : requests : cpu : 50m memory : 0.2G limits : cpu : 250m memory : 0.2G prometheus : jmx : enabled : false cp-kafka : enabled : true brokers : 1 imageTag : 7.1.3 podManagementPolicy : Parallel configurationOverrides : \"auto.create.topics.enable\" : false \"offsets.topic.replication.factor\" : 1 \"transaction.state.log.replication.factor\" : 1 \"transaction.state.log.min.isr\" : 1 resources : requests : cpu : 50m memory : 0.5G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false persistence : enabled : false cp-schema-registry : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-schema-registry\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.25G prometheus : jmx : enabled : false cp-kafka-connect : enabled : true replicaCount : 1 image : k3d-kpops-registry.localhost:12345/kafka-connect-jdbc imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-kafka-connect\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" heapOptions : \"-Xms256M -Xmx256\" resources : requests : cpu : 500m memory : 0.25G limits : cpu : 500m memory : 0.25G configurationOverrides : \"consumer.max.poll.records\" : \"10\" \"consumer.max.poll.interval.ms\" : \"900000\" \"config.storage.replication.factor\" : \"1\" \"offset.storage.replication.factor\" : \"1\" \"status.storage.replication.factor\" : \"1\" cp-schema-registry : url : http://k8kafka-cp-schema-registry:8081 prometheus : jmx : enabled : false cp-kafka-rest : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-rest\" heapOptions : \"-Xms256M -Xmx256M\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false cp-ksql-server : enabled : false cp-control-center : enabled : false","title":"Deploy Kafka"},{"location":"user/getting-started/setup/#deploy-streams-explorer","text":"Streams Explorer allows examining Apache Kafka data pipelines in a Kubernetes cluster including the inspection of schemas and monitoring of metrics. First, add the Helm repository: helm repo add streams-explorer https://bakdata.github.io/streams-explorer && \\ helm repo update Below you can find an example for the --values ./streams-explorer.yaml file configuring the deployment accordingly. Now, deploy the service: helm upgrade \\ --install \\ --version 0 .2.3 \\ --values ./streams-explorer.yaml \\ --namespace kpops \\ streams-explorer streams-explorer/streams-explorer Streams Explorer Helm chart values ( streams-explorer.yaml ) An example value configuration for Steams Explorer Helm chart. imageTag : \"v2.1.2\" config : K8S__deployment__cluster : true SCHEMAREGISTRY__url : http://k8kafka-cp-schema-registry.kpops.svc.cluster.local:8081 KAFKACONNECT__url : http://k8kafka-cp-kafka-connect.kpops.svc.cluster.local:8083 resources : requests : cpu : 200m memory : 300Mi limits : cpu : 200m memory : 300Mi","title":"Deploy Streams Explorer"},{"location":"user/getting-started/setup/#check-the-status-of-your-deployments","text":"Now we will check if all the pods are running in our namespace. You can list all pods in the namespace with this command: kubectl --namespace kpops get pods Then you should see the following out put in your terminal. NAME READY STATUS RESTARTS AGE k8kafka-cp-kafka-connect-8fc7d544f-8pjnt 1 /1 Running 0 15m k8kafka-cp-zookeeper-0 1 /1 Running 0 15m k8kafka-cp-kafka-0 1 /1 Running 0 15m k8kafka-cp-schema-registry-588f8c65db-jdwbq 1 /1 Running 0 15m k8kafka-cp-rest-6bbfd7b645-nwkf8 1 /1 Running 0 15m streams-explorer-54db878c67-s8wbz 1 /1 Running 0 15m Pay attention to the STATUS row. The pods should have a status of Running .","title":"Check the status of your deployments"},{"location":"user/getting-started/setup/#install-kpops","text":"KPOps comes as a PyPI package . You can install it with pip : pip install kpops","title":"Install KPOps"},{"location":"user/getting-started/teardown/","text":"Teardown resources \u00b6 KPOps teardown commands \u00b6 destroy : Removes Kubernetes resources. reset : Runs destroy , resets the states of Kafka Streams apps and resets offsets to zero. clean : Runs reset and removes all Kafka resources. KPOps-deployed pipeline \u00b6 The kpops CLI can be used to destroy a pipeline that was previously deployed with kpops . In case that doesn't work, the pipeline can always be taken down manually with helm (see section Infrastructure ). Export environment variables. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Navigate to the examples folder. Replace the <name-of-the-example-directory> with the example you want to tear down. For example the atm-fraud-detection . Remove the pipeline # Uncomment 1 line to either destroy, reset or clean. # poetry run kpops destroy <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops reset <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops clean <name-of-the-example-directory>/pipeline.yaml \\ --defaults <name-of-the-example-directory>/defaults \\ --config <name-of-the-example-directory>/config.yaml \\ --execute Infrastructure \u00b6 Delete namespace: kubectl delete namespace kpops Note In case kpops destroy is not working one can uninstall the pipeline services one by one. This is equivalent to running kpops destroy . In case a clean uninstall (like the one kpops clean does) is needed, one needs to also delete the topics and schemas created by deployment of the pipeline. Local cluster \u00b6 Delete local cluster: k3d cluster delete kpops Local image registry \u00b6 Delete local registry: k3d registry delete k3d-kpops-registry.localhost","title":"Teardown resources"},{"location":"user/getting-started/teardown/#teardown-resources","text":"","title":"Teardown resources"},{"location":"user/getting-started/teardown/#kpops-teardown-commands","text":"destroy : Removes Kubernetes resources. reset : Runs destroy , resets the states of Kafka Streams apps and resets offsets to zero. clean : Runs reset and removes all Kafka resources.","title":"KPOps teardown commands"},{"location":"user/getting-started/teardown/#kpops-deployed-pipeline","text":"The kpops CLI can be used to destroy a pipeline that was previously deployed with kpops . In case that doesn't work, the pipeline can always be taken down manually with helm (see section Infrastructure ). Export environment variables. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Navigate to the examples folder. Replace the <name-of-the-example-directory> with the example you want to tear down. For example the atm-fraud-detection . Remove the pipeline # Uncomment 1 line to either destroy, reset or clean. # poetry run kpops destroy <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops reset <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops clean <name-of-the-example-directory>/pipeline.yaml \\ --defaults <name-of-the-example-directory>/defaults \\ --config <name-of-the-example-directory>/config.yaml \\ --execute","title":"KPOps-deployed pipeline"},{"location":"user/getting-started/teardown/#infrastructure","text":"Delete namespace: kubectl delete namespace kpops Note In case kpops destroy is not working one can uninstall the pipeline services one by one. This is equivalent to running kpops destroy . In case a clean uninstall (like the one kpops clean does) is needed, one needs to also delete the topics and schemas created by deployment of the pipeline.","title":"Infrastructure"},{"location":"user/getting-started/teardown/#local-cluster","text":"Delete local cluster: k3d cluster delete kpops","title":"Local cluster"},{"location":"user/getting-started/teardown/#local-image-registry","text":"Delete local registry: k3d registry delete k3d-kpops-registry.localhost","title":"Local image registry"},{"location":"user/references/cli-commands/","text":"CLI usage \u00b6 Usage : $ kpops [ OPTIONS ] COMMAND [ ARGS ] Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : clean : Clean pipeline steps deploy : Deploy pipeline steps destroy : Destroy pipeline steps generate : Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. reset : Reset pipeline steps clean \u00b6 Clean pipeline steps Usage : $ kpops clean [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. deploy \u00b6 Deploy pipeline steps Usage : $ kpops deploy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. destroy \u00b6 Destroy pipeline steps Usage : $ kpops destroy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. generate \u00b6 Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. Usage : $ kpops generate [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --print-yaml / --no-print-yaml : Print enriched pipeline yaml definition [default: print-yaml] --save / --no-save : Save pipeline to yaml file [default: no-save] --out-path PATH : Path to file the yaml pipeline should be saved to [default: None] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. reset \u00b6 Reset pipeline steps Usage : $ reset [ OPTIONS ] PIPELINE_PATH COMPONENTS_MODULE Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"CLI usage"},{"location":"user/references/cli-commands/#cli-usage","text":"Usage : $ kpops [ OPTIONS ] COMMAND [ ARGS ] Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : clean : Clean pipeline steps deploy : Deploy pipeline steps destroy : Destroy pipeline steps generate : Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. reset : Reset pipeline steps","title":"CLI usage"},{"location":"user/references/cli-commands/#clean","text":"Clean pipeline steps Usage : $ kpops clean [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"clean"},{"location":"user/references/cli-commands/#deploy","text":"Deploy pipeline steps Usage : $ kpops deploy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"deploy"},{"location":"user/references/cli-commands/#destroy","text":"Destroy pipeline steps Usage : $ kpops destroy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"destroy"},{"location":"user/references/cli-commands/#generate","text":"Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. Usage : $ kpops generate [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --print-yaml / --no-print-yaml : Print enriched pipeline yaml definition [default: print-yaml] --save / --no-save : Save pipeline to yaml file [default: no-save] --out-path PATH : Path to file the yaml pipeline should be saved to [default: None] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"generate"},{"location":"user/references/cli-commands/#reset","text":"Reset pipeline steps Usage : $ reset [ OPTIONS ] PIPELINE_PATH COMPONENTS_MODULE Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"reset"},{"location":"user/references/variables/","text":"Variables \u00b6 Pipeline variables \u00b6 ${pipeline_name} : Concatenated path of the parent directory where pipeline.yaml is defined in. For instance, /data/pipelines/v1/pipeline.yaml , here the value for the variable would be data-pipelines-v1 . ${pipeline_name_<level>} : Similar to the previous variable, each <level> contains a part of the path to the pipeline.yaml file. Consider the previous example, ${pipeline_name_0} would be data , ${pipeline_name_1} would be pipelines , and ${pipeline_name_2} equals to v1 . Component specific variables \u00b6 ${component_type} : The type of the component ${component_name} : The name of the component Topic names depending on the component: ${topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_output_topic_name ${error_topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_error_topic_name An example for the content of the config.yaml to define your project-specific topic names is: topic_name_config : default_error_topic_name : \"${pipeline_name}-${component_type}-error-topic\" default_output_topic_name : \"${pipeline_name}-${component_type}-topic\" Environment Variables \u00b6 Variable Required Default Value Description KPOPS_PIPELINE_PATH Path to YAML with pipeline definition KPOPS_PIPELINE_BASE_DIR . Base directory to the pipelines KPOPS_DEFAULT_PATH defaults Path to defaults folder KPOPS_CONFIG_PATH config.yaml Path to the config.yaml file KPOPS_PIPELINE_STEPS None Comma separated list of steps to apply the command on","title":"Variables"},{"location":"user/references/variables/#variables","text":"","title":"Variables"},{"location":"user/references/variables/#pipeline-variables","text":"${pipeline_name} : Concatenated path of the parent directory where pipeline.yaml is defined in. For instance, /data/pipelines/v1/pipeline.yaml , here the value for the variable would be data-pipelines-v1 . ${pipeline_name_<level>} : Similar to the previous variable, each <level> contains a part of the path to the pipeline.yaml file. Consider the previous example, ${pipeline_name_0} would be data , ${pipeline_name_1} would be pipelines , and ${pipeline_name_2} equals to v1 .","title":"Pipeline variables"},{"location":"user/references/variables/#component-specific-variables","text":"${component_type} : The type of the component ${component_name} : The name of the component Topic names depending on the component: ${topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_output_topic_name ${error_topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_error_topic_name An example for the content of the config.yaml to define your project-specific topic names is: topic_name_config : default_error_topic_name : \"${pipeline_name}-${component_type}-error-topic\" default_output_topic_name : \"${pipeline_name}-${component_type}-topic\"","title":"Component specific variables"},{"location":"user/references/variables/#environment-variables","text":"Variable Required Default Value Description KPOPS_PIPELINE_PATH Path to YAML with pipeline definition KPOPS_PIPELINE_BASE_DIR . Base directory to the pipelines KPOPS_DEFAULT_PATH defaults Path to defaults folder KPOPS_CONFIG_PATH config.yaml Path to the config.yaml file KPOPS_PIPELINE_STEPS None Comma separated list of steps to apply the command on","title":"Environment Variables"}]}