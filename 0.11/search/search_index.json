{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"user/what-is-kpops/","text":"What is KPOps? \u00b6 KPOps is a CLI tool that allows deploying and destroying consecutive Kafka Streams applications, Producers, and Kafka Connectors using an easy-to-read and write pipeline definition. KPOps comes with various abstractions that can simplify configuring multiple pipelines and steps within pipelines by sharing configuration between different applications, like producers or streaming applications. In addition, Kafka apps and its resources can be reset or cleaned up again with a single command.","title":"What is KPOps?"},{"location":"user/what-is-kpops/#what-is-kpops","text":"KPOps is a CLI tool that allows deploying and destroying consecutive Kafka Streams applications, Producers, and Kafka Connectors using an easy-to-read and write pipeline definition. KPOps comes with various abstractions that can simplify configuring multiple pipelines and steps within pipelines by sharing configuration between different applications, like producers or streaming applications. In addition, Kafka apps and its resources can be reset or cleaned up again with a single command.","title":"What is KPOps?"},{"location":"user/examples/atm-fraud-pipeline/","text":"ATM fraud detection pipeline \u00b6 ATM fraud is a demo pipeline for ATM fraud detection. The original by Confluent is written in KSQL and outlined in this blogpost . The one used in this example is re-built from scratch using bakdata 's streams-bootstrap library. What this will demonstrate \u00b6 Deploying a PostgreSQL database using Helm Deploying a pipeline using kpops Destroying a pipeline using kpops Prerequisites \u00b6 Completed all steps in the setup . Setup and deployment \u00b6 PostgreSQL \u00b6 Deploy PostgreSQL using the Bitnami Helm chart: Add the helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami && \\ helm repo update Install the PostgreSQL with helm: helm upgrade --install -f ./postgresql.yaml \\ --namespace kpops \\ postgresql bitnami/postgresql PostgreSQL Example Helm chart values ( postgresql.yaml ) auth : database : app_db enablePostgresUser : true password : AppPassword postgresPassword : StrongPassword username : app1 primary : persistence : enabled : false existingClaim : postgresql-data-claim volumePermissions : enabled : true ATM fraud detection example pipeline setup \u00b6 Port forwarding \u00b6 Before we deploy the pipeline, we need to forward the ports of kafka-rest-proxy and kafka-connect . Run the following commands in two different terminals. kubectl port-forward --namespace kpops service/k8kafka-cp-rest 8082 :8082 kubectl port-forward --namespace kpops service/k8kafka-cp-kafka-connect 8083 :8083 Deploying the ATM fraud detection pipeline \u00b6 Export environment variables in your terminal: export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Deploy the pipeline poetry run kpops deploy ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be deployed correctly. Check if the deployment is successful \u00b6 You can use the Streams Explorer to see the deployed pipeline. To do so, port-forward the service in a separate terminal session using the command below: kubectl port-forward -n kpops service/streams-explorer 8080 :8080 After that open http://localhost:8080 in your browser. You should be able to see pipeline shown in the image below: An overview of ATM fraud pipeline shown in Streams Explorer Attention Kafka Connect needs some time to set up the connector. Moreover, Streams Explorer needs a while to scrape the information from Kafka connect. Therefore, it might take a bit until you see the whole graph. Teardown resources \u00b6 PostrgreSQL \u00b6 PostgreSQL can be uninstalled by running the following command: helm --namespace kpops uninstall postgresql ATM fraud pipeline \u00b6 Export environment variables in your terminal. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Remove the pipeline poetry run kpops clean ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --verbose \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be destroyed correctly. Attention If you face any issues destroying this example see Teardown for manual deletion. Common errors \u00b6 deploy fails: Read the error message. Try to correct the mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . Run deploy --dry-run to avoid havig to clean again. If an error is dropped, start over from step 1. If the dry-run is succesful, run deploy . clean fails: Read the error message. Try to correct the indicated mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . If clean fails, follow the steps in teardown .","title":"ATM fraud detection pipeline"},{"location":"user/examples/atm-fraud-pipeline/#atm-fraud-detection-pipeline","text":"ATM fraud is a demo pipeline for ATM fraud detection. The original by Confluent is written in KSQL and outlined in this blogpost . The one used in this example is re-built from scratch using bakdata 's streams-bootstrap library.","title":"ATM fraud detection pipeline"},{"location":"user/examples/atm-fraud-pipeline/#what-this-will-demonstrate","text":"Deploying a PostgreSQL database using Helm Deploying a pipeline using kpops Destroying a pipeline using kpops","title":"What this will demonstrate"},{"location":"user/examples/atm-fraud-pipeline/#prerequisites","text":"Completed all steps in the setup .","title":"Prerequisites"},{"location":"user/examples/atm-fraud-pipeline/#setup-and-deployment","text":"","title":"Setup and deployment"},{"location":"user/examples/atm-fraud-pipeline/#postgresql","text":"Deploy PostgreSQL using the Bitnami Helm chart: Add the helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami && \\ helm repo update Install the PostgreSQL with helm: helm upgrade --install -f ./postgresql.yaml \\ --namespace kpops \\ postgresql bitnami/postgresql PostgreSQL Example Helm chart values ( postgresql.yaml ) auth : database : app_db enablePostgresUser : true password : AppPassword postgresPassword : StrongPassword username : app1 primary : persistence : enabled : false existingClaim : postgresql-data-claim volumePermissions : enabled : true","title":"PostgreSQL"},{"location":"user/examples/atm-fraud-pipeline/#atm-fraud-detection-example-pipeline-setup","text":"","title":"ATM fraud detection example pipeline setup"},{"location":"user/examples/atm-fraud-pipeline/#port-forwarding","text":"Before we deploy the pipeline, we need to forward the ports of kafka-rest-proxy and kafka-connect . Run the following commands in two different terminals. kubectl port-forward --namespace kpops service/k8kafka-cp-rest 8082 :8082 kubectl port-forward --namespace kpops service/k8kafka-cp-kafka-connect 8083 :8083","title":"Port forwarding"},{"location":"user/examples/atm-fraud-pipeline/#deploying-the-atm-fraud-detection-pipeline","text":"Export environment variables in your terminal: export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Deploy the pipeline poetry run kpops deploy ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be deployed correctly.","title":"Deploying the ATM fraud detection pipeline"},{"location":"user/examples/atm-fraud-pipeline/#check-if-the-deployment-is-successful","text":"You can use the Streams Explorer to see the deployed pipeline. To do so, port-forward the service in a separate terminal session using the command below: kubectl port-forward -n kpops service/streams-explorer 8080 :8080 After that open http://localhost:8080 in your browser. You should be able to see pipeline shown in the image below: An overview of ATM fraud pipeline shown in Streams Explorer Attention Kafka Connect needs some time to set up the connector. Moreover, Streams Explorer needs a while to scrape the information from Kafka connect. Therefore, it might take a bit until you see the whole graph.","title":"Check if the deployment is successful"},{"location":"user/examples/atm-fraud-pipeline/#teardown-resources","text":"","title":"Teardown resources"},{"location":"user/examples/atm-fraud-pipeline/#postrgresql","text":"PostgreSQL can be uninstalled by running the following command: helm --namespace kpops uninstall postgresql","title":"PostrgreSQL"},{"location":"user/examples/atm-fraud-pipeline/#atm-fraud-pipeline","text":"Export environment variables in your terminal. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Remove the pipeline poetry run kpops clean ./examples/bakdata/atm-fraud-detection/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/atm-fraud-detection/defaults \\ --config ./examples/bakdata/atm-fraud-detection/config.yaml \\ --verbose \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be destroyed correctly. Attention If you face any issues destroying this example see Teardown for manual deletion.","title":"ATM fraud pipeline"},{"location":"user/examples/atm-fraud-pipeline/#common-errors","text":"deploy fails: Read the error message. Try to correct the mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . Run deploy --dry-run to avoid havig to clean again. If an error is dropped, start over from step 1. If the dry-run is succesful, run deploy . clean fails: Read the error message. Try to correct the indicated mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . If clean fails, follow the steps in teardown .","title":"Common errors"},{"location":"user/getting-started/quick-start/","text":"Quick Start \u00b6 Word-count \u00b6 Word-count is a demo pipeline which consists of a producer producing words to Kafka, a Kafka streams app counting the number of times each word occurs and finally a Redis database into which the words are exported. What this will demonstrate \u00b6 Deploying a Redis database using Helm Deploying a pipeline using kpops Destroying a pipeline using kpops Prerequisites \u00b6 Completed all steps in the setup . Setup and deployment \u00b6 Redis \u00b6 Deploy Redis using the Bitnami Helm chart: Add the Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami && \\ helm repo update Install Redis with Helm: helm upgrade --install -f ./values-redis.yaml \\ --namespace kpops \\ redis bitnami/redis Redis example Helm chart values ( values-redis.yaml ) architecture : standalone auth : enabled : false master : count : 1 configuration : \"databases 1\" image : tag : 7.0.8 Word-count example pipeline setup \u00b6 Port forwarding \u00b6 Before we deploy the pipeline, we need to forward the ports of kafka-rest-proxy and kafka-connect . Run the following commands in two different terminals. kubectl port-forward --namespace kpops service/k8kafka-cp-rest 8082 :8082 kubectl port-forward --namespace kpops service/k8kafka-cp-kafka-connect 8083 :8083 Deploying the Word-count pipeline \u00b6 Copy the configuration from the kpops-examples repository into kpops>examples>bakdata>word-count like so: kpops \u251c\u2500\u2500 examples | \u251c\u2500\u2500 bakdata | | \u251c\u2500\u2500 word-count | | | \u251c\u2500\u2500 config.yaml | | | \u251c\u2500\u2500 defaults | | | \u2502 \u2514\u2500\u2500 defaults.yaml | | | \u2514\u2500\u2500 pipeline.yaml | | | Export environment variables in your terminal: export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Deploy the pipeline kpops deploy ./examples/bakdata/word-count/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/word-count/defaults \\ --config ./examples/bakdata/word-count/config.yaml \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be deployed correctly. Check if the deployment is successful \u00b6 You can use the Streams Explorer to inspect the deployed pipeline. To do so, port-forward the service in a separate terminal session using the command below: kubectl port-forward -n kpops service/streams-explorer 8080 :8080 After that open http://localhost:8080 in your browser. You should be able to see pipeline shown in the image below: An overview of Word-count pipeline shown in Streams Explorer Attention Kafka Connect needs some time to set up the connector. Moreover, Streams Explorer needs a while to scrape the information from Kafka Connect. Therefore, it might take a bit until you see the whole graph. Teardown resources \u00b6 Redis \u00b6 Redis can be uninstalled by running the following command: helm --namespace kpops uninstall redis Word-count pipeline \u00b6 Export environment variables in your terminal. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Remove the pipeline kpops clean ./examples/bakdata/word-count/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/word-count/defaults \\ --config ./examples/bakdata/word-count/config.yaml \\ --verbose \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be destroyed correctly. Attention If you face any issues destroying this example see Teardown for manual deletion. Common errors \u00b6 deploy fails: Read the error message. Try to correct the mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . Run deploy --dry-run to avoid havig to clean again. If an error is dropped, start over from step 1. If the dry-run is succesful, run deploy . clean fails: Read the error message. Try to correct the indicated mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . If clean fails, follow the steps in teardown .","title":"Quick Start"},{"location":"user/getting-started/quick-start/#quick-start","text":"","title":"Quick Start"},{"location":"user/getting-started/quick-start/#word-count","text":"Word-count is a demo pipeline which consists of a producer producing words to Kafka, a Kafka streams app counting the number of times each word occurs and finally a Redis database into which the words are exported.","title":"Word-count"},{"location":"user/getting-started/quick-start/#what-this-will-demonstrate","text":"Deploying a Redis database using Helm Deploying a pipeline using kpops Destroying a pipeline using kpops","title":"What this will demonstrate"},{"location":"user/getting-started/quick-start/#prerequisites","text":"Completed all steps in the setup .","title":"Prerequisites"},{"location":"user/getting-started/quick-start/#setup-and-deployment","text":"","title":"Setup and deployment"},{"location":"user/getting-started/quick-start/#redis","text":"Deploy Redis using the Bitnami Helm chart: Add the Helm repository: helm repo add bitnami https://charts.bitnami.com/bitnami && \\ helm repo update Install Redis with Helm: helm upgrade --install -f ./values-redis.yaml \\ --namespace kpops \\ redis bitnami/redis Redis example Helm chart values ( values-redis.yaml ) architecture : standalone auth : enabled : false master : count : 1 configuration : \"databases 1\" image : tag : 7.0.8","title":"Redis"},{"location":"user/getting-started/quick-start/#word-count-example-pipeline-setup","text":"","title":"Word-count example pipeline setup"},{"location":"user/getting-started/quick-start/#port-forwarding","text":"Before we deploy the pipeline, we need to forward the ports of kafka-rest-proxy and kafka-connect . Run the following commands in two different terminals. kubectl port-forward --namespace kpops service/k8kafka-cp-rest 8082 :8082 kubectl port-forward --namespace kpops service/k8kafka-cp-kafka-connect 8083 :8083","title":"Port forwarding"},{"location":"user/getting-started/quick-start/#deploying-the-word-count-pipeline","text":"Copy the configuration from the kpops-examples repository into kpops>examples>bakdata>word-count like so: kpops \u251c\u2500\u2500 examples | \u251c\u2500\u2500 bakdata | | \u251c\u2500\u2500 word-count | | | \u251c\u2500\u2500 config.yaml | | | \u251c\u2500\u2500 defaults | | | \u2502 \u2514\u2500\u2500 defaults.yaml | | | \u2514\u2500\u2500 pipeline.yaml | | | Export environment variables in your terminal: export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Deploy the pipeline kpops deploy ./examples/bakdata/word-count/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/word-count/defaults \\ --config ./examples/bakdata/word-count/config.yaml \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be deployed correctly.","title":"Deploying the Word-count pipeline"},{"location":"user/getting-started/quick-start/#check-if-the-deployment-is-successful","text":"You can use the Streams Explorer to inspect the deployed pipeline. To do so, port-forward the service in a separate terminal session using the command below: kubectl port-forward -n kpops service/streams-explorer 8080 :8080 After that open http://localhost:8080 in your browser. You should be able to see pipeline shown in the image below: An overview of Word-count pipeline shown in Streams Explorer Attention Kafka Connect needs some time to set up the connector. Moreover, Streams Explorer needs a while to scrape the information from Kafka Connect. Therefore, it might take a bit until you see the whole graph.","title":"Check if the deployment is successful"},{"location":"user/getting-started/quick-start/#teardown-resources","text":"","title":"Teardown resources"},{"location":"user/getting-started/quick-start/#redis_1","text":"Redis can be uninstalled by running the following command: helm --namespace kpops uninstall redis","title":"Redis"},{"location":"user/getting-started/quick-start/#word-count-pipeline","text":"Export environment variables in your terminal. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Remove the pipeline kpops clean ./examples/bakdata/word-count/pipeline.yaml \\ --pipeline-base-dir ./examples \\ --defaults ./examples/bakdata/word-count/defaults \\ --config ./examples/bakdata/word-count/config.yaml \\ --verbose \\ --execute Note You can use the --dry-run flag instead of the --execute flag and check the logs if your pipeline will be destroyed correctly. Attention If you face any issues destroying this example see Teardown for manual deletion.","title":"Word-count pipeline"},{"location":"user/getting-started/quick-start/#common-errors","text":"deploy fails: Read the error message. Try to correct the mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . Run deploy --dry-run to avoid havig to clean again. If an error is dropped, start over from step 1. If the dry-run is succesful, run deploy . clean fails: Read the error message. Try to correct the indicated mistakes if there were any. Likely the configuration is not correct or the port-forwarding is not working as intended. Run clean . If clean fails, follow the steps in teardown .","title":"Common errors"},{"location":"user/getting-started/setup/","text":"Setup KPOps \u00b6 In this part, you will set up KPOps. This includes: optionally creating a local Kubernetes cluster running Apache Kafka and Confluent's Schema Registry installing KPOps Prerequisites \u00b6 k3d (Version 5.4.6+) and Docker (Version >= v20.10.5) or an existing Kubernetes cluster (>= 1.21.0) kubectl (Compatible with server version 1.21.0) Helm (Version 3.8.0+) Setup Kubernetes with k3d \u00b6 If you don't have access to an existing Kubernetes cluster, this section will guide you through creating a local cluster. We recommend the lightweight Kubernetes distribution k3s for this. k3d is a wrapper around k3s in Docker that lets you get started fast. You can install k3d with its installation script: wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/v5.4.6/install.sh | bash For other ways of installing k3d, you can have a look at their installation guide . The Kafka deployment needs a modified Docker image. In that case the image is built and pushed to a Docker registry that holds it. If you do not have access to an existing Docker registry, you can use k3d's Docker registry: k3d registry create kpops-registry.localhost --port 12345 Now you can create a new cluster called kpops that uses the previously created Docker registry: k3d cluster create kpops --k3s-arg \"--no-deploy=traefik@server:*\" --registry-use k3d-kpops-registry.localhost:12345 Note Creating a new k3d cluster automatically configures kubectl to connect to the local cluster by modifying your ~/.kube/config . In case you manually set the KUBECONFIG variable or don't want k3d to modify your config, k3d offers many other options . You can check the cluster status with kubectl get pods -n kube-system . If all returned elements have a STATUS of Running or Completed , then the cluster is up and running. Deploy Kafka \u00b6 Kafka is an open-source data streaming platform. More information about Kafka can be found in the documentation . To deploy Kafka, this guide uses Confluent's Helm chart . To allow connectivity to other systems Kafka Connect needs to be extended with drivers. You can install a JDBC driver for Kafka Connect by creating a new Docker image: Create a Dockerfile with the following content: FROM confluentinc/cp-kafka-connect:7.1.3 RUN confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.6.0 Build and push the modified image to your private Docker registry: docker build . --tag localhost:12345/kafka-connect-jdbc:7.1.3 && \\ docker push localhost:12345/kafka-connect-jdbc:7.1.3 Detailed instructions on building, tagging and pushing a docker image can be found in Docker docs . Add Confluent's Helm chart repository and update the index: helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/ && helm repo update Install Kafka, Zookeeper, Confluent's Schema Registry, Kafka Rest Proxy, and Kafka Connect. A single Helm chart installs all five components. Below you can find an example for the --values ./kafka.yaml file configuring the deployment accordingly. Deploy the services: helm upgrade \\ --install \\ --version 0 .6.1 \\ --values ./kafka.yaml \\ --namespace kpops \\ --create-namespace \\ --wait \\ k8kafka confluentinc/cp-helm-charts Kafka Helm chart values ( kafka.yaml ) An example value configuration for Confluent's Helm chart. This configuration deploys a single Kafka Broker, a Schema Registry, Zookeeper, Kafka Rest Proxy, and Kafka Connect with minimal resources. cp-zookeeper : enabled : true servers : 1 imageTag : 7.1.3 heapOptions : \"-Xms124M -Xmx124M\" overrideGroupId : k8kafka fullnameOverride : \"k8kafka-cp-zookeeper\" resources : requests : cpu : 50m memory : 0.2G limits : cpu : 250m memory : 0.2G prometheus : jmx : enabled : false cp-kafka : enabled : true brokers : 1 imageTag : 7.1.3 podManagementPolicy : Parallel configurationOverrides : \"auto.create.topics.enable\" : false \"offsets.topic.replication.factor\" : 1 \"transaction.state.log.replication.factor\" : 1 \"transaction.state.log.min.isr\" : 1 \"confluent.metrics.reporter.topic.replicas\" : 1 resources : requests : cpu : 50m memory : 0.5G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false persistence : enabled : false cp-schema-registry : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-schema-registry\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.25G prometheus : jmx : enabled : false cp-kafka-connect : enabled : true replicaCount : 1 image : k3d-kpops-registry.localhost:12345/kafka-connect-jdbc imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-kafka-connect\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" heapOptions : \"-Xms256M -Xmx256M\" resources : requests : cpu : 500m memory : 0.25G limits : cpu : 500m memory : 0.25G configurationOverrides : \"consumer.max.poll.records\" : \"10\" \"consumer.max.poll.interval.ms\" : \"900000\" \"config.storage.replication.factor\" : \"1\" \"offset.storage.replication.factor\" : \"1\" \"status.storage.replication.factor\" : \"1\" cp-schema-registry : url : http://k8kafka-cp-schema-registry:8081 prometheus : jmx : enabled : false cp-kafka-rest : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-rest\" heapOptions : \"-Xms256M -Xmx256M\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false cp-ksql-server : enabled : false cp-control-center : enabled : false Deploy Streams Explorer \u00b6 Streams Explorer allows examining Apache Kafka data pipelines in a Kubernetes cluster including the inspection of schemas and monitoring of metrics. First, add the Helm repository: helm repo add streams-explorer https://bakdata.github.io/streams-explorer && \\ helm repo update Below you can find an example for the --values ./streams-explorer.yaml file configuring the deployment accordingly. Now, deploy the service: helm upgrade \\ --install \\ --version 0 .2.3 \\ --values ./streams-explorer.yaml \\ --namespace kpops \\ streams-explorer streams-explorer/streams-explorer Streams Explorer Helm chart values ( streams-explorer.yaml ) An example value configuration for Steams Explorer Helm chart. imageTag : \"v2.1.2\" config : K8S__deployment__cluster : true SCHEMAREGISTRY__url : http://k8kafka-cp-schema-registry.kpops.svc.cluster.local:8081 KAFKACONNECT__url : http://k8kafka-cp-kafka-connect.kpops.svc.cluster.local:8083 resources : requests : cpu : 200m memory : 300Mi limits : cpu : 200m memory : 300Mi Check the status of your deployments \u00b6 Now we will check if all the pods are running in our namespace. You can list all pods in the namespace with this command: kubectl --namespace kpops get pods Then you should see the following out put in your terminal. NAME READY STATUS RESTARTS AGE k8kafka-cp-kafka-connect-8fc7d544f-8pjnt 1 /1 Running 0 15m k8kafka-cp-zookeeper-0 1 /1 Running 0 15m k8kafka-cp-kafka-0 1 /1 Running 0 15m k8kafka-cp-schema-registry-588f8c65db-jdwbq 1 /1 Running 0 15m k8kafka-cp-rest-6bbfd7b645-nwkf8 1 /1 Running 0 15m streams-explorer-54db878c67-s8wbz 1 /1 Running 0 15m Pay attention to the STATUS row. The pods should have a status of Running . Install KPOps \u00b6 KPOps comes as a PyPI package . You can install it with pip : pip install kpops","title":"Setup KPOps"},{"location":"user/getting-started/setup/#setup-kpops","text":"In this part, you will set up KPOps. This includes: optionally creating a local Kubernetes cluster running Apache Kafka and Confluent's Schema Registry installing KPOps","title":"Setup KPOps"},{"location":"user/getting-started/setup/#prerequisites","text":"k3d (Version 5.4.6+) and Docker (Version >= v20.10.5) or an existing Kubernetes cluster (>= 1.21.0) kubectl (Compatible with server version 1.21.0) Helm (Version 3.8.0+)","title":"Prerequisites"},{"location":"user/getting-started/setup/#setup-kubernetes-with-k3d","text":"If you don't have access to an existing Kubernetes cluster, this section will guide you through creating a local cluster. We recommend the lightweight Kubernetes distribution k3s for this. k3d is a wrapper around k3s in Docker that lets you get started fast. You can install k3d with its installation script: wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/v5.4.6/install.sh | bash For other ways of installing k3d, you can have a look at their installation guide . The Kafka deployment needs a modified Docker image. In that case the image is built and pushed to a Docker registry that holds it. If you do not have access to an existing Docker registry, you can use k3d's Docker registry: k3d registry create kpops-registry.localhost --port 12345 Now you can create a new cluster called kpops that uses the previously created Docker registry: k3d cluster create kpops --k3s-arg \"--no-deploy=traefik@server:*\" --registry-use k3d-kpops-registry.localhost:12345 Note Creating a new k3d cluster automatically configures kubectl to connect to the local cluster by modifying your ~/.kube/config . In case you manually set the KUBECONFIG variable or don't want k3d to modify your config, k3d offers many other options . You can check the cluster status with kubectl get pods -n kube-system . If all returned elements have a STATUS of Running or Completed , then the cluster is up and running.","title":"Setup Kubernetes with k3d"},{"location":"user/getting-started/setup/#deploy-kafka","text":"Kafka is an open-source data streaming platform. More information about Kafka can be found in the documentation . To deploy Kafka, this guide uses Confluent's Helm chart . To allow connectivity to other systems Kafka Connect needs to be extended with drivers. You can install a JDBC driver for Kafka Connect by creating a new Docker image: Create a Dockerfile with the following content: FROM confluentinc/cp-kafka-connect:7.1.3 RUN confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.6.0 Build and push the modified image to your private Docker registry: docker build . --tag localhost:12345/kafka-connect-jdbc:7.1.3 && \\ docker push localhost:12345/kafka-connect-jdbc:7.1.3 Detailed instructions on building, tagging and pushing a docker image can be found in Docker docs . Add Confluent's Helm chart repository and update the index: helm repo add confluentinc https://confluentinc.github.io/cp-helm-charts/ && helm repo update Install Kafka, Zookeeper, Confluent's Schema Registry, Kafka Rest Proxy, and Kafka Connect. A single Helm chart installs all five components. Below you can find an example for the --values ./kafka.yaml file configuring the deployment accordingly. Deploy the services: helm upgrade \\ --install \\ --version 0 .6.1 \\ --values ./kafka.yaml \\ --namespace kpops \\ --create-namespace \\ --wait \\ k8kafka confluentinc/cp-helm-charts Kafka Helm chart values ( kafka.yaml ) An example value configuration for Confluent's Helm chart. This configuration deploys a single Kafka Broker, a Schema Registry, Zookeeper, Kafka Rest Proxy, and Kafka Connect with minimal resources. cp-zookeeper : enabled : true servers : 1 imageTag : 7.1.3 heapOptions : \"-Xms124M -Xmx124M\" overrideGroupId : k8kafka fullnameOverride : \"k8kafka-cp-zookeeper\" resources : requests : cpu : 50m memory : 0.2G limits : cpu : 250m memory : 0.2G prometheus : jmx : enabled : false cp-kafka : enabled : true brokers : 1 imageTag : 7.1.3 podManagementPolicy : Parallel configurationOverrides : \"auto.create.topics.enable\" : false \"offsets.topic.replication.factor\" : 1 \"transaction.state.log.replication.factor\" : 1 \"transaction.state.log.min.isr\" : 1 \"confluent.metrics.reporter.topic.replicas\" : 1 resources : requests : cpu : 50m memory : 0.5G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false persistence : enabled : false cp-schema-registry : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-schema-registry\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.25G prometheus : jmx : enabled : false cp-kafka-connect : enabled : true replicaCount : 1 image : k3d-kpops-registry.localhost:12345/kafka-connect-jdbc imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-kafka-connect\" overrideGroupId : k8kafka kafka : bootstrapServers : \"PLAINTEXT://k8kafka-cp-kafka-headless:9092\" heapOptions : \"-Xms256M -Xmx256M\" resources : requests : cpu : 500m memory : 0.25G limits : cpu : 500m memory : 0.25G configurationOverrides : \"consumer.max.poll.records\" : \"10\" \"consumer.max.poll.interval.ms\" : \"900000\" \"config.storage.replication.factor\" : \"1\" \"offset.storage.replication.factor\" : \"1\" \"status.storage.replication.factor\" : \"1\" cp-schema-registry : url : http://k8kafka-cp-schema-registry:8081 prometheus : jmx : enabled : false cp-kafka-rest : enabled : true imageTag : 7.1.3 fullnameOverride : \"k8kafka-cp-rest\" heapOptions : \"-Xms256M -Xmx256M\" resources : requests : cpu : 50m memory : 0.25G limits : cpu : 250m memory : 0.5G prometheus : jmx : enabled : false cp-ksql-server : enabled : false cp-control-center : enabled : false","title":"Deploy Kafka"},{"location":"user/getting-started/setup/#deploy-streams-explorer","text":"Streams Explorer allows examining Apache Kafka data pipelines in a Kubernetes cluster including the inspection of schemas and monitoring of metrics. First, add the Helm repository: helm repo add streams-explorer https://bakdata.github.io/streams-explorer && \\ helm repo update Below you can find an example for the --values ./streams-explorer.yaml file configuring the deployment accordingly. Now, deploy the service: helm upgrade \\ --install \\ --version 0 .2.3 \\ --values ./streams-explorer.yaml \\ --namespace kpops \\ streams-explorer streams-explorer/streams-explorer Streams Explorer Helm chart values ( streams-explorer.yaml ) An example value configuration for Steams Explorer Helm chart. imageTag : \"v2.1.2\" config : K8S__deployment__cluster : true SCHEMAREGISTRY__url : http://k8kafka-cp-schema-registry.kpops.svc.cluster.local:8081 KAFKACONNECT__url : http://k8kafka-cp-kafka-connect.kpops.svc.cluster.local:8083 resources : requests : cpu : 200m memory : 300Mi limits : cpu : 200m memory : 300Mi","title":"Deploy Streams Explorer"},{"location":"user/getting-started/setup/#check-the-status-of-your-deployments","text":"Now we will check if all the pods are running in our namespace. You can list all pods in the namespace with this command: kubectl --namespace kpops get pods Then you should see the following out put in your terminal. NAME READY STATUS RESTARTS AGE k8kafka-cp-kafka-connect-8fc7d544f-8pjnt 1 /1 Running 0 15m k8kafka-cp-zookeeper-0 1 /1 Running 0 15m k8kafka-cp-kafka-0 1 /1 Running 0 15m k8kafka-cp-schema-registry-588f8c65db-jdwbq 1 /1 Running 0 15m k8kafka-cp-rest-6bbfd7b645-nwkf8 1 /1 Running 0 15m streams-explorer-54db878c67-s8wbz 1 /1 Running 0 15m Pay attention to the STATUS row. The pods should have a status of Running .","title":"Check the status of your deployments"},{"location":"user/getting-started/setup/#install-kpops","text":"KPOps comes as a PyPI package . You can install it with pip : pip install kpops","title":"Install KPOps"},{"location":"user/getting-started/teardown/","text":"Teardown resources \u00b6 KPOps teardown commands \u00b6 destroy : Removes Kubernetes resources. reset : Runs destroy , resets the states of Kafka Streams apps and resets offsets to zero. clean : Runs reset and removes all Kafka resources. KPOps-deployed pipeline \u00b6 The kpops CLI can be used to destroy a pipeline that was previously deployed with kpops . In case that doesn't work, the pipeline can always be taken down manually with helm (see section Infrastructure ). Export environment variables. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Navigate to the examples folder. Replace the <name-of-the-example-directory> with the example you want to tear down. For example the atm-fraud-detection . Remove the pipeline # Uncomment 1 line to either destroy, reset or clean. # poetry run kpops destroy <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops reset <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops clean <name-of-the-example-directory>/pipeline.yaml \\ --defaults <name-of-the-example-directory>/defaults \\ --config <name-of-the-example-directory>/config.yaml \\ --execute Infrastructure \u00b6 Delete namespace: kubectl delete namespace kpops Note In case kpops destroy is not working one can uninstall the pipeline services one by one. This is equivalent to running kpops destroy . In case a clean uninstall (like the one kpops clean does) is needed, one needs to also delete the topics and schemas created by deployment of the pipeline. Local cluster \u00b6 Delete local cluster: k3d cluster delete kpops Local image registry \u00b6 Delete local registry: k3d registry delete k3d-kpops-registry.localhost","title":"Teardown resources"},{"location":"user/getting-started/teardown/#teardown-resources","text":"","title":"Teardown resources"},{"location":"user/getting-started/teardown/#kpops-teardown-commands","text":"destroy : Removes Kubernetes resources. reset : Runs destroy , resets the states of Kafka Streams apps and resets offsets to zero. clean : Runs reset and removes all Kafka resources.","title":"KPOps teardown commands"},{"location":"user/getting-started/teardown/#kpops-deployed-pipeline","text":"The kpops CLI can be used to destroy a pipeline that was previously deployed with kpops . In case that doesn't work, the pipeline can always be taken down manually with helm (see section Infrastructure ). Export environment variables. export DOCKER_REGISTRY = bakdata && \\ export NAMESPACE = kpops Navigate to the examples folder. Replace the <name-of-the-example-directory> with the example you want to tear down. For example the atm-fraud-detection . Remove the pipeline # Uncomment 1 line to either destroy, reset or clean. # poetry run kpops destroy <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops reset <name-of-the-example-directory>/pipeline.yaml \\ # poetry run kpops clean <name-of-the-example-directory>/pipeline.yaml \\ --defaults <name-of-the-example-directory>/defaults \\ --config <name-of-the-example-directory>/config.yaml \\ --execute","title":"KPOps-deployed pipeline"},{"location":"user/getting-started/teardown/#infrastructure","text":"Delete namespace: kubectl delete namespace kpops Note In case kpops destroy is not working one can uninstall the pipeline services one by one. This is equivalent to running kpops destroy . In case a clean uninstall (like the one kpops clean does) is needed, one needs to also delete the topics and schemas created by deployment of the pipeline.","title":"Infrastructure"},{"location":"user/getting-started/teardown/#local-cluster","text":"Delete local cluster: k3d cluster delete kpops","title":"Local cluster"},{"location":"user/getting-started/teardown/#local-image-registry","text":"Delete local registry: k3d registry delete k3d-kpops-registry.localhost","title":"Local image registry"},{"location":"user/references/cli-commands/","text":"CLI usage \u00b6 Usage : $ kpops [ OPTIONS ] COMMAND [ ARGS ] Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : clean : Clean pipeline steps deploy : Deploy pipeline steps destroy : Destroy pipeline steps generate : Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. reset : Reset pipeline steps clean \u00b6 Clean pipeline steps Usage : $ kpops clean [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. deploy \u00b6 Deploy pipeline steps Usage : $ kpops deploy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. destroy \u00b6 Destroy pipeline steps Usage : $ kpops destroy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. generate \u00b6 Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. Usage : $ kpops generate [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --print-yaml / --no-print-yaml : Print enriched pipeline yaml definition [default: print-yaml] --save / --no-save : Save pipeline to yaml file [default: no-save] --out-path PATH : Path to file the yaml pipeline should be saved to [default: None] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit. reset \u00b6 Reset pipeline steps Usage : $ reset [ OPTIONS ] PIPELINE_PATH COMPONENTS_MODULE Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"CLI usage"},{"location":"user/references/cli-commands/#cli-usage","text":"Usage : $ kpops [ OPTIONS ] COMMAND [ ARGS ] Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : clean : Clean pipeline steps deploy : Deploy pipeline steps destroy : Destroy pipeline steps generate : Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. reset : Reset pipeline steps","title":"CLI usage"},{"location":"user/references/cli-commands/#clean","text":"Clean pipeline steps Usage : $ kpops clean [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"clean"},{"location":"user/references/cli-commands/#deploy","text":"Deploy pipeline steps Usage : $ kpops deploy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"deploy"},{"location":"user/references/cli-commands/#destroy","text":"Destroy pipeline steps Usage : $ kpops destroy [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"destroy"},{"location":"user/references/cli-commands/#generate","text":"Enriches pipelines steps with defaults. The output is used as input for the deploy/destroy/... commands. Usage : $ kpops generate [ OPTIONS ] PIPELINE_PATH [ COMPONENTS_MODULE ] Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --print-yaml / --no-print-yaml : Print enriched pipeline yaml definition [default: print-yaml] --save / --no-save : Save pipeline to yaml file [default: no-save] --out-path PATH : Path to file the yaml pipeline should be saved to [default: None] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"generate"},{"location":"user/references/cli-commands/#reset","text":"Reset pipeline steps Usage : $ reset [ OPTIONS ] PIPELINE_PATH COMPONENTS_MODULE Arguments : pipeline_path FILE : Path to YAML with pipeline definition [env var: KPOPS_PIPELINE_PATH] [default: None] [required] components_module [COMPONENTS_MODULE] : Custom Python module containing your project-specific components [default: None] Options : --pipeline-base-dir DIRECTORY : Base directory to the pipelines (default is current working directory) [env var: KPOPS_PIPELINE_BASE_DIR] [default: .] --defaults DIRECTORY : Path to defaults folder [env var: KPOPS_DEFAULT_PATH] [default: defaults] --config FILE : Path to the config.yaml file [env var: KPOPS_CONFIG_PATH] [default: config.yaml] --steps TEXT : Comma separated list of steps to apply the command on [env var: KPOPS_PIPELINE_STEPS] [default: None] --dry-run / --execute : Whether to dry run the command or execute it [default: dry-run] --verbose / --no-verbose : [default: no-verbose] --help : Show this message and exit.","title":"reset"},{"location":"user/references/components/","text":"Components \u00b6 This section explains the different components of KPOps, their usage and configuration. KubernetesApp \u00b6 Usage \u00b6 Can be used to deploy any app in Kubernetes using Helm, for example, a REST service that serves Kafka data. Configuration \u00b6 pipeline.yaml # Base Kubernetes App - type : kubernetes-app name : kubernetes-app # required namespace : namespace # required # `app` contains application-specific settings, hence it does not have a rigid # structure. The fields below are just an example. app : # required image : exampleImage # Example debug : false # Example commandLine : {} # Example # Topic(s) from which the component will read input from : topics : # required ${pipeline_name}-input-topic : type : input # required # role: topic-role # only used if type is `extra` or `extra-pattern` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration repoConfig : repositoryName : my-repo # required url : https://bakdata.github.io/ # required repoAuthFlags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : 1.0.0 # Helm chart version Operations \u00b6 deploy \u00b6 Deploy using Helm. destroy \u00b6 Uninstall Helm release. reset \u00b6 Do nothing. clean \u00b6 Do nothing. KafkaApp \u00b6 Sub class of KubernetesApp . Usage \u00b6 Defines a streams-bootstrap component Should not be used in pipeline.yaml as the component can be defined as either a StreamsApp or a Producer Often used in defaults.yaml Configuration \u00b6 pipeline.yaml # Base component for Kafka-based components. # Producer or streaming apps should inherit from this class. - type : kafka-app # required name : kafka-app # required namespace : namespace # required # `app` can contain application-specific settings, hence the user is free to # add the key-value pairs they need. app : # required streams : # required brokers : ${broker} # required schemaRegistryUrl : ${schema_registry_url} nameOverride : override-with-this-name # kafka-app-specific imageTag : 1.0.0 # Example values that are shared between streams-app and producer-app # Topic(s) from which the component will read input # from: # Not advised to ever be used in `kafka-app` as it isn't supported by # `producer` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` # Currently KPOps only supports Avro schemas. keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration, the default value is given here repoConfig : repositoryName : bakdata-streams-bootstrap # required url : https://bakdata.github.io/streams-bootstrap/ # required repo_auth_flags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"2.7.0\" # Helm chart version Operations \u00b6 deploy \u00b6 In addition to KubernetesApp's deploy : Create topics if provided (optional) Submit Avro schemas to the registry if provided (optional) destroy \u00b6 Refer to KubernetesApp . reset \u00b6 Do nothing. clean \u00b6 Do nothing. StreamsApp \u00b6 Sub class of KafkaApp . Usage \u00b6 Configures a streams-bootstrap Kafka Streams app Configuration \u00b6 pipeline.yaml # StreamsApp component that configures a streams bootstrap app. # More documentation on StreamsApp: https://github.com/bakdata/streams-bootstrap - type : streams-app # required name : streams-app # required namespace : namespace # required # No arbitrary keys are allowed under `app`here # Allowed configs: # https://github.com/bakdata/streams-bootstrap/tree/master/charts/streams-app app : # required # Streams Bootstrap streams section streams : # required, streams-app-specific brokers : ${broker} # required schemaRegistryUrl : ${schema_registry_url} inputTopics : - topic1 - topic2 outputTopic : output-topic inputPattern : input-pattern extraInputTopics : input_role1 : - input_topic1 - input_topic2 input_role2 : - input_topic3 - input_topic4 extraInputPatterns : pattern_role1 : input_pattern1 extraOutputTopics : output_role1 : output_topic1 output_role2 : output_topic2 errorTopic : error-topic config : my.streams.config : my.value nameOverride : override-with-this-name # streams-app-specific autoscaling : # streams-app-specific consumerGroup : consumer-group # required lagThreshold : 0 # Average target value to trigger scaling actions. enabled : false # Whether to enable auto-scaling using KEDA. # This is the interval to check each trigger on. # https://keda.sh/docs/2.7/concepts/scaling-deployments/#pollinginterval pollingInterval : 30 # The period to wait after the last trigger reported active before scaling # the resource back to 0. https://keda.sh/docs/2.7/concepts/scaling-deployments/#cooldownperiod cooldownPeriod : 300 # The offset reset policy for the consumer if the the consumer group is # not yet subscribed to a partition. offsetResetPolicy : earliest # This setting is passed to the HPA definition that KEDA will create for a # given resource and holds the maximum number of replicas of the target resouce. # https://keda.sh/docs/2.7/concepts/scaling-deployments/#maxreplicacount maxReplicas : 1 # If this property is set, KEDA will scale the resource down to this # number of replicas. # https://keda.sh/docs/2.7/concepts/scaling-deployments/#idlereplicacount idleReplicas : 0 topics : # List of auto-generated Kafka Streams topics used by the streams app. - topic1 - topic2 # Topic(s) from which the component will read input from : topics : # required ${pipeline_name}-input-topic : type : input # required # role: topic-role # only used if type is `extra` or `extra-pattern` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration, the default value is given here repoConfig : repositoryName : bakdata-streams-bootstrap # required url : https://bakdata.github.io/streams-bootstrap/ # required repo_auth_flags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"2.7.0\" # Helm chart version Operations \u00b6 deploy \u00b6 Refer to KafkaApp . destroy \u00b6 Refer to KafkaApp . reset \u00b6 Reset the consumer group offsets Reset Kafka Streams state clean \u00b6 Reset Kafka Streams state Delete the app's output topics Delete all associated schemas in the Schema Registry Delete the consumer group Producer \u00b6 Sub class of KafkaApp . Usage \u00b6 Configures a streams-bootstrap Kafka producer app Configuration \u00b6 pipeline.yaml # Holds configuration to use as values for the streams bootstrap produce Helm # chart. # More documentation on ProducersApp: # https://github.com/bakdata/streams-bootstrap - type : producer name : producer # required namespace : namespace # required # No arbitrary keys are allowed under `app`here # Allowed configs: # https://github.com/bakdata/streams-bootstrap/tree/master/charts/producer-app app : # required streams : # required, producer-specific brokers : ${broker} # required schemaRegistryUrl : ${schema_registry_url} outputTopic : output_topic extraOutputTopics : output_role1 : output_topic1 output_role2 : output_topic2 nameOverride : override-with-this-name # kafka-app-specific # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration, the default value is given here repoConfig : repositoryName : bakdata-streams-bootstrap # required url : https://bakdata.github.io/streams-bootstrap/ # required repo_auth_flags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"2.7.0\" # Helm chart version Operations \u00b6 deploy \u00b6 Refer to KafkaApp . destroy \u00b6 Refer to KafkaApp . reset \u00b6 Do nothing, producers are stateless. clean \u00b6 Delete the output topics of the Kafka producer Delete all associated schemas in the Schema Registry KafkaSinkConnector \u00b6 Sub class of KafkaConnector Usage \u00b6 Lets other systems pull data from Apache Kafka. Configuration \u00b6 pipeline.yaml # Kafka sink connector - type : kafka-sink-connector name : kafka-sink-connector # required, namespace : namespace # required, clean up jobs for the connector will run here # `app` contains application-specific settings, hence it does not have a rigid # structure. The fields below are just an example. Extensive documentation on # sink connectors: https://kafka.apache.org/documentation/#sinkconnectconfigs app : # required tasks.max : 1 # Topic(s) from which the component will read input from : topics : # required ${pipeline_name}-input-topic : type : input # required # role: topic-role # only used if type is `extra` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : error # required keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration for resetter repoConfig : repositoryName : my-repo # required url : https://bakdata.github.io/kafka-connect-resetter/ # required repoAuthFlags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"1.0.4\" # Helm chart version # Overriding Kafka Connect Resetter Helm values. E.g. to override the # Image Tag etc. resetterValues : imageTag : 1.2.3 offsetTopic : offset_topic Operations \u00b6 deploy \u00b6 Add the sink connector to the Kafka Connect cluster Create the output topics if provided (optional) Register schemas in the Schema Registry if provided (optional) destroy \u00b6 The associated sink connector is removed. reset \u00b6 Reset the consumer group offsets. clean \u00b6 Delete associated consumer group Delete configured error topics KafkaSourceConnector \u00b6 Sub class of KafkaConnector Usage \u00b6 Manages source connectors in your Kafka Connect cluster. Configuration \u00b6 pipeline.yaml # Kafka source connector - type : kafka-source-connector name : kafka-source-connector # required namespace : namespace # required, clean up jobs for the connector will run here # `app` contains application-specific settings, hence it does not have a rigid # structure. The fields below are just an example. Extensive documentation on # sink connectors: https://kafka.apache.org/documentation/#sourceconnectconfigs app : # required key1 : tasks.max : 1 errors.tolerance : none # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration repoConfig : repositoryName : my-repo # required url : https://bakdata.github.io/kafka-connect-resetter/ # required repoAuthFlags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"1.0.4\" # Helm chart version # Overriding Kafka Connect Resetter Helm values. E.g. to override the # Image Tag etc. resetterValues : imageTag : 1.2.3 offsetTopic : offset_topic Operations \u00b6 deploy \u00b6 Add the source connector to the Kafka Connect cluster Create the output topics if provided (optional) Register schemas in the Schema registry if provided (optional) destroy \u00b6 Remove the source connector from the Kafka Connect cluster. reset \u00b6 Delete state associated with the connector clean \u00b6 Delete all associated output topics Delete all associated schemas in the Schema Registry Delete state associated with the connector","title":"Components"},{"location":"user/references/components/#components","text":"This section explains the different components of KPOps, their usage and configuration.","title":"Components"},{"location":"user/references/components/#kubernetesapp","text":"","title":"KubernetesApp"},{"location":"user/references/components/#usage","text":"Can be used to deploy any app in Kubernetes using Helm, for example, a REST service that serves Kafka data.","title":"Usage"},{"location":"user/references/components/#configuration","text":"pipeline.yaml # Base Kubernetes App - type : kubernetes-app name : kubernetes-app # required namespace : namespace # required # `app` contains application-specific settings, hence it does not have a rigid # structure. The fields below are just an example. app : # required image : exampleImage # Example debug : false # Example commandLine : {} # Example # Topic(s) from which the component will read input from : topics : # required ${pipeline_name}-input-topic : type : input # required # role: topic-role # only used if type is `extra` or `extra-pattern` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration repoConfig : repositoryName : my-repo # required url : https://bakdata.github.io/ # required repoAuthFlags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : 1.0.0 # Helm chart version","title":"Configuration"},{"location":"user/references/components/#operations","text":"","title":"Operations"},{"location":"user/references/components/#deploy","text":"Deploy using Helm.","title":"deploy"},{"location":"user/references/components/#destroy","text":"Uninstall Helm release.","title":"destroy"},{"location":"user/references/components/#reset","text":"Do nothing.","title":"reset"},{"location":"user/references/components/#clean","text":"Do nothing.","title":"clean"},{"location":"user/references/components/#kafkaapp","text":"Sub class of KubernetesApp .","title":"KafkaApp"},{"location":"user/references/components/#usage_1","text":"Defines a streams-bootstrap component Should not be used in pipeline.yaml as the component can be defined as either a StreamsApp or a Producer Often used in defaults.yaml","title":"Usage"},{"location":"user/references/components/#configuration_1","text":"pipeline.yaml # Base component for Kafka-based components. # Producer or streaming apps should inherit from this class. - type : kafka-app # required name : kafka-app # required namespace : namespace # required # `app` can contain application-specific settings, hence the user is free to # add the key-value pairs they need. app : # required streams : # required brokers : ${broker} # required schemaRegistryUrl : ${schema_registry_url} nameOverride : override-with-this-name # kafka-app-specific imageTag : 1.0.0 # Example values that are shared between streams-app and producer-app # Topic(s) from which the component will read input # from: # Not advised to ever be used in `kafka-app` as it isn't supported by # `producer` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` # Currently KPOps only supports Avro schemas. keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration, the default value is given here repoConfig : repositoryName : bakdata-streams-bootstrap # required url : https://bakdata.github.io/streams-bootstrap/ # required repo_auth_flags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"2.7.0\" # Helm chart version","title":"Configuration"},{"location":"user/references/components/#operations_1","text":"","title":"Operations"},{"location":"user/references/components/#deploy_1","text":"In addition to KubernetesApp's deploy : Create topics if provided (optional) Submit Avro schemas to the registry if provided (optional)","title":"deploy"},{"location":"user/references/components/#destroy_1","text":"Refer to KubernetesApp .","title":"destroy"},{"location":"user/references/components/#reset_1","text":"Do nothing.","title":"reset"},{"location":"user/references/components/#clean_1","text":"Do nothing.","title":"clean"},{"location":"user/references/components/#streamsapp","text":"Sub class of KafkaApp .","title":"StreamsApp"},{"location":"user/references/components/#usage_2","text":"Configures a streams-bootstrap Kafka Streams app","title":"Usage"},{"location":"user/references/components/#configuration_2","text":"pipeline.yaml # StreamsApp component that configures a streams bootstrap app. # More documentation on StreamsApp: https://github.com/bakdata/streams-bootstrap - type : streams-app # required name : streams-app # required namespace : namespace # required # No arbitrary keys are allowed under `app`here # Allowed configs: # https://github.com/bakdata/streams-bootstrap/tree/master/charts/streams-app app : # required # Streams Bootstrap streams section streams : # required, streams-app-specific brokers : ${broker} # required schemaRegistryUrl : ${schema_registry_url} inputTopics : - topic1 - topic2 outputTopic : output-topic inputPattern : input-pattern extraInputTopics : input_role1 : - input_topic1 - input_topic2 input_role2 : - input_topic3 - input_topic4 extraInputPatterns : pattern_role1 : input_pattern1 extraOutputTopics : output_role1 : output_topic1 output_role2 : output_topic2 errorTopic : error-topic config : my.streams.config : my.value nameOverride : override-with-this-name # streams-app-specific autoscaling : # streams-app-specific consumerGroup : consumer-group # required lagThreshold : 0 # Average target value to trigger scaling actions. enabled : false # Whether to enable auto-scaling using KEDA. # This is the interval to check each trigger on. # https://keda.sh/docs/2.7/concepts/scaling-deployments/#pollinginterval pollingInterval : 30 # The period to wait after the last trigger reported active before scaling # the resource back to 0. https://keda.sh/docs/2.7/concepts/scaling-deployments/#cooldownperiod cooldownPeriod : 300 # The offset reset policy for the consumer if the the consumer group is # not yet subscribed to a partition. offsetResetPolicy : earliest # This setting is passed to the HPA definition that KEDA will create for a # given resource and holds the maximum number of replicas of the target resouce. # https://keda.sh/docs/2.7/concepts/scaling-deployments/#maxreplicacount maxReplicas : 1 # If this property is set, KEDA will scale the resource down to this # number of replicas. # https://keda.sh/docs/2.7/concepts/scaling-deployments/#idlereplicacount idleReplicas : 0 topics : # List of auto-generated Kafka Streams topics used by the streams app. - topic1 - topic2 # Topic(s) from which the component will read input from : topics : # required ${pipeline_name}-input-topic : type : input # required # role: topic-role # only used if type is `extra` or `extra-pattern` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration, the default value is given here repoConfig : repositoryName : bakdata-streams-bootstrap # required url : https://bakdata.github.io/streams-bootstrap/ # required repo_auth_flags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"2.7.0\" # Helm chart version","title":"Configuration"},{"location":"user/references/components/#operations_2","text":"","title":"Operations"},{"location":"user/references/components/#deploy_2","text":"Refer to KafkaApp .","title":"deploy"},{"location":"user/references/components/#destroy_2","text":"Refer to KafkaApp .","title":"destroy"},{"location":"user/references/components/#reset_2","text":"Reset the consumer group offsets Reset Kafka Streams state","title":"reset"},{"location":"user/references/components/#clean_2","text":"Reset Kafka Streams state Delete the app's output topics Delete all associated schemas in the Schema Registry Delete the consumer group","title":"clean"},{"location":"user/references/components/#producer","text":"Sub class of KafkaApp .","title":"Producer"},{"location":"user/references/components/#usage_3","text":"Configures a streams-bootstrap Kafka producer app","title":"Usage"},{"location":"user/references/components/#configuration_3","text":"pipeline.yaml # Holds configuration to use as values for the streams bootstrap produce Helm # chart. # More documentation on ProducersApp: # https://github.com/bakdata/streams-bootstrap - type : producer name : producer # required namespace : namespace # required # No arbitrary keys are allowed under `app`here # Allowed configs: # https://github.com/bakdata/streams-bootstrap/tree/master/charts/producer-app app : # required streams : # required, producer-specific brokers : ${broker} # required schemaRegistryUrl : ${schema_registry_url} outputTopic : output_topic extraOutputTopics : output_role1 : output_topic1 output_role2 : output_topic2 nameOverride : override-with-this-name # kafka-app-specific # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration, the default value is given here repoConfig : repositoryName : bakdata-streams-bootstrap # required url : https://bakdata.github.io/streams-bootstrap/ # required repo_auth_flags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"2.7.0\" # Helm chart version","title":"Configuration"},{"location":"user/references/components/#operations_3","text":"","title":"Operations"},{"location":"user/references/components/#deploy_3","text":"Refer to KafkaApp .","title":"deploy"},{"location":"user/references/components/#destroy_3","text":"Refer to KafkaApp .","title":"destroy"},{"location":"user/references/components/#reset_3","text":"Do nothing, producers are stateless.","title":"reset"},{"location":"user/references/components/#clean_3","text":"Delete the output topics of the Kafka producer Delete all associated schemas in the Schema Registry","title":"clean"},{"location":"user/references/components/#kafkasinkconnector","text":"Sub class of KafkaConnector","title":"KafkaSinkConnector"},{"location":"user/references/components/#usage_4","text":"Lets other systems pull data from Apache Kafka.","title":"Usage"},{"location":"user/references/components/#configuration_4","text":"pipeline.yaml # Kafka sink connector - type : kafka-sink-connector name : kafka-sink-connector # required, namespace : namespace # required, clean up jobs for the connector will run here # `app` contains application-specific settings, hence it does not have a rigid # structure. The fields below are just an example. Extensive documentation on # sink connectors: https://kafka.apache.org/documentation/#sinkconnectconfigs app : # required tasks.max : 1 # Topic(s) from which the component will read input from : topics : # required ${pipeline_name}-input-topic : type : input # required # role: topic-role # only used if type is `extra` # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : error # required keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration for resetter repoConfig : repositoryName : my-repo # required url : https://bakdata.github.io/kafka-connect-resetter/ # required repoAuthFlags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"1.0.4\" # Helm chart version # Overriding Kafka Connect Resetter Helm values. E.g. to override the # Image Tag etc. resetterValues : imageTag : 1.2.3 offsetTopic : offset_topic","title":"Configuration"},{"location":"user/references/components/#operations_4","text":"","title":"Operations"},{"location":"user/references/components/#deploy_4","text":"Add the sink connector to the Kafka Connect cluster Create the output topics if provided (optional) Register schemas in the Schema Registry if provided (optional)","title":"deploy"},{"location":"user/references/components/#destroy_4","text":"The associated sink connector is removed.","title":"destroy"},{"location":"user/references/components/#reset_4","text":"Reset the consumer group offsets.","title":"reset"},{"location":"user/references/components/#clean_4","text":"Delete associated consumer group Delete configured error topics","title":"clean"},{"location":"user/references/components/#kafkasourceconnector","text":"Sub class of KafkaConnector","title":"KafkaSourceConnector"},{"location":"user/references/components/#usage_5","text":"Manages source connectors in your Kafka Connect cluster.","title":"Usage"},{"location":"user/references/components/#configuration_5","text":"pipeline.yaml # Kafka source connector - type : kafka-source-connector name : kafka-source-connector # required namespace : namespace # required, clean up jobs for the connector will run here # `app` contains application-specific settings, hence it does not have a rigid # structure. The fields below are just an example. Extensive documentation on # sink connectors: https://kafka.apache.org/documentation/#sourceconnectconfigs app : # required key1 : tasks.max : 1 errors.tolerance : none # Topic(s) into which the component will write output to : topics : # required ${pipeline_name}-output-topic : type : output # required # role: topic-role # only used if type is `extra` keySchema : key-schema # must implement SchemaProvider to use valueSchema : value-schema partitions_count : 1 replication_factor : 1 configs : # https://kafka.apache.org/documentation/#topicconfigs cleanup.policy : compact models : # SchemaProvider is initiated with the values given here model : model # Pipeline prefix that will prefix every component name. If you wish to not # have any prefix you can specify an empty string. prefix : ${pipeline_name}- # Helm repository configuration repoConfig : repositoryName : my-repo # required url : https://bakdata.github.io/kafka-connect-resetter/ # required repoAuthFlags : username : user password : pass ca_file : /home/user/path/to/ca-file insecure_skip_tls_verify : false version : \"1.0.4\" # Helm chart version # Overriding Kafka Connect Resetter Helm values. E.g. to override the # Image Tag etc. resetterValues : imageTag : 1.2.3 offsetTopic : offset_topic","title":"Configuration"},{"location":"user/references/components/#operations_5","text":"","title":"Operations"},{"location":"user/references/components/#deploy_5","text":"Add the source connector to the Kafka Connect cluster Create the output topics if provided (optional) Register schemas in the Schema registry if provided (optional)","title":"deploy"},{"location":"user/references/components/#destroy_5","text":"Remove the source connector from the Kafka Connect cluster.","title":"destroy"},{"location":"user/references/components/#reset_5","text":"Delete state associated with the connector","title":"reset"},{"location":"user/references/components/#clean_5","text":"Delete all associated output topics Delete all associated schemas in the Schema Registry Delete state associated with the connector","title":"clean"},{"location":"user/references/config/","text":"Configuration \u00b6 KPOps reads most of a pipeline 's configuration that is unrelated to its components from config.yaml . Consider enabling KPOps' editor integration feature to enjoy the benefits of autocompletion and validation when configuring your pipeline. To learn about any of the available settings, take a look at the example below. config.yaml # CONFIGURATION # # The path to the folder containing the defaults.yaml file and the environment # defaults files. # REQUIRED defaults_path : /path/to/defaults # The environment you want to generate and deploy the pipeline to. Suffix your # environment files with this value (e.g. defaults_development.yaml for # environment=development). # REQUIRED environment : development # The Kafka broker address. # REQUIRED broker : \"http://localhost:9092\" # The name of the defaults file and the prefix of the defaults environment file. defaults_filename_prefix : defaults # Configures topic names. topic_name_config : # Configures the value for the variable ${output_topic_name} default_output_topic_name : ${pipeline_name}-${component_name} # Configures the value for the variable ${error_topic_name} default_error_topic_name : ${pipeline_name}-${component_name}-error # Address of the Schema Registry schema_registry_url : \"http://localhost:8081\" # Address of the Kafka REST Proxy. kafka_rest_host : \"http://localhost:8082\" # Address of Kafka Connect. kafka_connect_host : \"http://localhost:8083\" # The timeout in seconds that specifies when actions like deletion or deploy # timeout. timeout : 300 # Flag for `helm upgrade --install`. # Create the release namespace if not present. create_namespace : false # Global flags for Helm. helm_config : # Set the name of the kubeconfig context. (--kube-context) context : name # Run Helm in Debug mode. debug : false # Configure Helm Diff. helm_diff_config : # Enable Helm Diff. enable : true # Set of keys that should not be checked. ignore : - name - imageTag # Whether to retain clean up jobs in the cluster or uninstall the, after # completion. retain_clean_jobs : false","title":"Configuration"},{"location":"user/references/config/#configuration","text":"KPOps reads most of a pipeline 's configuration that is unrelated to its components from config.yaml . Consider enabling KPOps' editor integration feature to enjoy the benefits of autocompletion and validation when configuring your pipeline. To learn about any of the available settings, take a look at the example below. config.yaml # CONFIGURATION # # The path to the folder containing the defaults.yaml file and the environment # defaults files. # REQUIRED defaults_path : /path/to/defaults # The environment you want to generate and deploy the pipeline to. Suffix your # environment files with this value (e.g. defaults_development.yaml for # environment=development). # REQUIRED environment : development # The Kafka broker address. # REQUIRED broker : \"http://localhost:9092\" # The name of the defaults file and the prefix of the defaults environment file. defaults_filename_prefix : defaults # Configures topic names. topic_name_config : # Configures the value for the variable ${output_topic_name} default_output_topic_name : ${pipeline_name}-${component_name} # Configures the value for the variable ${error_topic_name} default_error_topic_name : ${pipeline_name}-${component_name}-error # Address of the Schema Registry schema_registry_url : \"http://localhost:8081\" # Address of the Kafka REST Proxy. kafka_rest_host : \"http://localhost:8082\" # Address of Kafka Connect. kafka_connect_host : \"http://localhost:8083\" # The timeout in seconds that specifies when actions like deletion or deploy # timeout. timeout : 300 # Flag for `helm upgrade --install`. # Create the release namespace if not present. create_namespace : false # Global flags for Helm. helm_config : # Set the name of the kubeconfig context. (--kube-context) context : name # Run Helm in Debug mode. debug : false # Configure Helm Diff. helm_diff_config : # Enable Helm Diff. enable : true # Set of keys that should not be checked. ignore : - name - imageTag # Whether to retain clean up jobs in the cluster or uninstall the, after # completion. retain_clean_jobs : false","title":"Configuration"},{"location":"user/references/editor-integration/","text":"Editor integration \u00b6 KPOps provides JSON schemas that enable autocompletion and validation for some of the files that the user must work with. Supported files \u00b6 pipeline.yaml config.yaml Usage \u00b6 Install the yaml-language-server in your editor of choice. (requires LSP support) Configure the extension with the settings below. settings.json { \"yaml.schemas\" : { \"https://bakdata.github.io/kpops/latest/schema/config.json\" : \"config.yaml\" , \"https://bakdata.github.io/kpops/latest/schema/pipeline.json\" : \"pipeline.yaml\" } }","title":"Editor integration"},{"location":"user/references/editor-integration/#editor-integration","text":"KPOps provides JSON schemas that enable autocompletion and validation for some of the files that the user must work with.","title":"Editor integration"},{"location":"user/references/editor-integration/#supported-files","text":"pipeline.yaml config.yaml","title":"Supported files"},{"location":"user/references/editor-integration/#usage","text":"Install the yaml-language-server in your editor of choice. (requires LSP support) Configure the extension with the settings below. settings.json { \"yaml.schemas\" : { \"https://bakdata.github.io/kpops/latest/schema/config.json\" : \"config.yaml\" , \"https://bakdata.github.io/kpops/latest/schema/pipeline.json\" : \"pipeline.yaml\" } }","title":"Usage"},{"location":"user/references/variables/","text":"Variables \u00b6 Pipeline variables \u00b6 ${pipeline_name} : Concatenated path of the parent directory where pipeline.yaml is defined in. For instance, /data/pipelines/v1/pipeline.yaml , here the value for the variable would be data-pipelines-v1 . ${pipeline_name_<level>} : Similar to the previous variable, each <level> contains a part of the path to the pipeline.yaml file. Consider the previous example, ${pipeline_name_0} would be data , ${pipeline_name_1} would be pipelines , and ${pipeline_name_2} equals to v1 . Component specific variables \u00b6 ${component_type} : The type of the component ${component_name} : The name of the component Topic names depending on the component: ${topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_output_topic_name ${error_topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_error_topic_name An example for the content of the config.yaml to define your project-specific topic names is: topic_name_config : default_error_topic_name : \"${pipeline_name}-${component_type}-error-topic\" default_output_topic_name : \"${pipeline_name}-${component_type}-topic\"","title":"Variables"},{"location":"user/references/variables/#variables","text":"","title":"Variables"},{"location":"user/references/variables/#pipeline-variables","text":"${pipeline_name} : Concatenated path of the parent directory where pipeline.yaml is defined in. For instance, /data/pipelines/v1/pipeline.yaml , here the value for the variable would be data-pipelines-v1 . ${pipeline_name_<level>} : Similar to the previous variable, each <level> contains a part of the path to the pipeline.yaml file. Consider the previous example, ${pipeline_name_0} would be data , ${pipeline_name_1} would be pipelines , and ${pipeline_name_2} equals to v1 .","title":"Pipeline variables"},{"location":"user/references/variables/#component-specific-variables","text":"${component_type} : The type of the component ${component_name} : The name of the component Topic names depending on the component: ${topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_output_topic_name ${error_topic_name} : You can define the value using the config.yaml and the field: topic_name_config.default_error_topic_name An example for the content of the config.yaml to define your project-specific topic names is: topic_name_config : default_error_topic_name : \"${pipeline_name}-${component_type}-error-topic\" default_output_topic_name : \"${pipeline_name}-${component_type}-topic\"","title":"Component specific variables"}]}